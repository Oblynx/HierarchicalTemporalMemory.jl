<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Examples · HierarchicalTemporalMemory.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://oblynx.github.io/HierarchicalTemporalMemory.jl/examples/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><script src="../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="HierarchicalTemporalMemory.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="HierarchicalTemporalMemory.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">HierarchicalTemporalMemory.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Home</span><ul><li><a class="tocitem" href="../">Hierarchical Temporal Memory</a></li><li><a class="tocitem" href="../poster/">JuliaCon 2020 poster</a></li></ul></li><li class="is-active"><a class="tocitem" href>Examples</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Timeseries-prediction:-power-consumption-in-a-gym"><span>Timeseries prediction: power consumption in a gym</span></a></li><li><a class="tocitem" href="#HTM-model"><span>HTM model</span></a></li><li><a class="tocitem" href="#Prediction-results"><span>Prediction results</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Examples</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Oblynx/HierarchicalTemporalMemory.jl/blob/master/docs/src/examples.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h1><h1 id="Timeseries-prediction:-power-consumption-in-a-gym"><a class="docs-heading-anchor" href="#Timeseries-prediction:-power-consumption-in-a-gym">Timeseries prediction: power consumption in a gym</a><a id="Timeseries-prediction:-power-consumption-in-a-gym-1"></a><a class="docs-heading-anchor-permalink" href="#Timeseries-prediction:-power-consumption-in-a-gym" title="Permalink"></a></h1><p><a href="https://github.com/Oblynx/HierarchicalTemporalMemory.jl/blob/master/test/temporal_memory_test.jl">This example</a> reproduces a simple upstream experiment by making 1-step-ahead predictions on a single regularly-sampled time series of a building&#39;s power consumption.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Although HTM is about modelling the brain, this example won&#39;t look into that at all and look at it simply as a predictive model.</p></div></div><p>The time series exhibits daily and weekly patterns, but also mean shifts and non-ergodicity. Here&#39;s what a small part of the time series looks like:</p><p><img src="../assets/tshotgym.svg" alt="power consumption timeseries"/></p><p>We&#39;ll use this data in this example. Follow along <a href="https://github.com/Oblynx/HierarchicalTemporalMemory.jl/blob/master/test/temporal_memory_test.jl">at the source</a></p><h2 id="HTM-model"><a class="docs-heading-anchor" href="#HTM-model">HTM model</a><a id="HTM-model-1"></a><a class="docs-heading-anchor-permalink" href="#HTM-model" title="Permalink"></a></h2><p><img src="../assets/htm_predict_pipeline.png" alt="htm model"/></p><h3 id="Encoding"><a class="docs-heading-anchor" href="#Encoding">Encoding</a><a id="Encoding-1"></a><a class="docs-heading-anchor-permalink" href="#Encoding" title="Permalink"></a></h3><p>An HTM model doesn&#39;t understand numbers for input. It needs large binary vectors that represent other neurons firing. These are called Sparse Distributed Representations (SDRs). Transforming input data into SDRs is called encoding and the important point is to maintain the semantics of the input space under this transformation: if <code>3.1kW</code> is considered to be very similar to <code>3.2kW</code>, then the corresponding SDRs should also be very similar. Correspondingly the SDR for <code>5kW</code> should probably have no similarity.</p><p>So what is SDR similarity? As binary vectors, &quot;similarity&quot; is how many 1s they have at the same places, or, the inner product of 2 SDRs, or <code>bitwise(AND)</code> then <code>reduce(+)</code>.</p><pre><code class="language-julia hljs">similarity(x,y)= sum(x .&amp; y);
A= [0,0,1,0,1,0,1,0];
B= [0,0,1,0,0,0,1,0];

similarity(A,B)

C= [1,1,1,0,0,1,0,1];
similarity(A,C)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1</code></pre><p>In this case the encoder is [<code>encode_simpleArithmetic</code>] and linearly maps a range of real numbers <code>[a,b]</code> to shifting bit patterns within a fixed length SDR. If the range is <code>[0,1]</code>, it could look like:</p><pre><code class="nohighlight hljs">0   -&gt; 1110000000
0.2 -&gt; 0011100000
...
1   -&gt; 0000000111</code></pre><h3 id="Spatial-Pooler"><a class="docs-heading-anchor" href="#Spatial-Pooler">Spatial Pooler</a><a id="Spatial-Pooler-1"></a><a class="docs-heading-anchor-permalink" href="#Spatial-Pooler" title="Permalink"></a></h3><p>The <a href="../reference/#HierarchicalTemporalMemory.SpatialPooler"><code>SpatialPooler</code></a> is a learning algorithm that decorrelates the features of an input space, producing a Sparse Distributed Representation (SDR) of the input space.</p><p>It&#39;s called <code>SpatialPooler</code> because input patterns that share a large number of co-active neurons (i.e., that are spatially similar) are grouped together into a common output representation. It is designed to achieve a set of <em>computational properties</em> that support further downstream computations with SDRs, including:</p><ul><li>preserving topology of the input space by mapping similar inputs to similar outputs</li><li>continuously adapting to changing statistics of the input stream</li><li>forming fixed sparsity representations</li><li>being robust to noise</li><li>being fault tolerant</li></ul><p><a href="https://www.frontiersin.org/articles/10.3389/fncom.2017.00111/full">Source</a></p><p>We can evaluate the &quot;mapping property&quot; of the spatial pooler, or if it manages to represent the similarities of the input space in the output:</p><p><img src="../assets/sp3.svg" alt="spatial pooler mapping property evaluation"/></p><h3 id="Temporal-memory"><a class="docs-heading-anchor" href="#Temporal-memory">Temporal memory</a><a id="Temporal-memory-1"></a><a class="docs-heading-anchor-permalink" href="#Temporal-memory" title="Permalink"></a></h3><p>After the SDRs have been normalized comes the contextualizing and predictive step, the <a href="../reference/#HierarchicalTemporalMemory.TemporalMemory"><code>TemporalMemory</code></a>. The <code>TemporalMemory</code> learns to predict sequences of input SDRs. It represents each input symbol <em>in the temporal context of the symbols that come before it</em> in the sequence and <em>predicts its own activation</em> in the following time step.</p><p>This prediction is the TM&#39;s output, another SDR that we must decode to bring it back out from the model&#39;s internal representation. Decoding however is more complicated than encoding, because the only information we have to associate the TM&#39;s activations with the input symbols is that they represent the input symbol in a temporal context. What might help us is discovering a function that maps internal representations to the input symbols that they should represent.</p><p>We can train a simple learning algorithm (eg 1-layer neural network) to decode the TM, by using the actual encodings as ground truth. If we want to predict <code>k</code> time steps into the future, then at time <code>t</code> we update the learning algorithm with error signal</p><p><span>$e_t = D(Π_{t-k}) - u_t$</span></p><p>where <code>Π</code> is the TM&#39;s prediction, an SDR, <code>D(Π)</code> is the decoding of the prediction, and <code>u</code> is the input.</p><p>Here, the decoder <a href="@ref"><code>SDRClassifier</code></a> actually outputs a probability distribution over all the encoder&#39;s classes.</p><h2 id="Prediction-results"><a class="docs-heading-anchor" href="#Prediction-results">Prediction results</a><a id="Prediction-results-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction-results" title="Permalink"></a></h2><p><img src="../assets/tm1.svg" alt="prediction results"/></p><ul><li>Blue: original time series</li><li>Red: prediction 1 step ahead</li></ul><p>To make the prediction above proper dimensions were chosen for the algorithms, but their various tuning parameters weren&#39;t tuned a lot. The error metric is <a href="https://en.wikipedia.org/wiki/Mean_absolute_scaled_error">MASE</a>, calculated on a 10-day sliding window.</p><p>The learning parts of the system are:</p><ul><li>Spatial Pooler</li><li>Temporal Memory</li><li>Decoder</li></ul><p>In this example they&#39;re all learning simultaneously, but the spatial pooler&#39;s learning can be considered an optimization. The important learning components are the temporal memory and the deoder.</p><p>Before the system has learned anything the decoder outputs a more or less static value in the middle of the encoding range. Gradually, the most frequent patterns of the time series start being recognised: the daily fluctuations. Weekends take more time, as what counts is the number of times a pattern has been seen. But is the prediction good in any case?</p><p>After about 600 steps the HTM model does a bit better than simply predicting the current value for the next value and starts hitting the daily peaks. From step 1200 on the frequency of small unpredictable events increases and the accuracy starts dropping. However the mean shift at 1750 is handled gracefully and doesn&#39;t increase the error further. The HTM <em>adapts to the new statistics.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../poster/">« JuliaCon 2020 poster</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Monday 7 February 2022 22:46">Monday 7 February 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
