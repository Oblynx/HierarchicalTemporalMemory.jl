"""
SPParams{Nin,Nsp} holds the algorithm parameters for a spatial pooler with nomenclature
similar to [source](https://www.frontiersin.org/articles/10.3389/fncom.2017.00111/full)

The dimension parameters are problem-specific and should be the first to be specified.

The tuning parameters have sensible defaults, which should be .
All gated features are enabled by default.

# Parameters

## Dimensions
- `száµ¢â‚™`, `szâ‚›â‚š`: input/output dimensions
- `Î³`: receptive field radius (how large an input area an output minicolumn maps to)

## Algorithm tuning
- `s`: average output sparsity
- `prob_synapse`: probability for each element of the `száµ¢â‚™ Ã— szâ‚›â‚š` space to be a synapse.
  Elements that roll below this value don't form a synapse and don't get a permanence value.
  If this is very low, the proximal synapses matrix can become sparse.
- `Î¸_permanence01`: synapse permanence connection threshold
- `pâº_01`,`pâ»_01`: synapse permanence adaptation rate (see [`ProximalSynapses`](@ref))
- `Î¸_stimulus_activate`: minicolumn absolute activation threshold
- `Tboost`: boosting mechanism's moving average filter period
- `Î²`: boosting strength

## Feature gates
- `enable_local_inhibit`
- `enable_learning`
- `enable_boosting`
"""
@with_kw struct SPParams{Nin,Nsp}
  # dimensions
  száµ¢â‚™::NTuple{Nin,Int}     = (32,32); @assert all(száµ¢â‚™.>0)
  szâ‚›â‚š::NTuple{Nsp,Int}     = (64,64); @assert all(szâ‚›â‚š.>0)
  Î³::Int                    = 6;       @assert Î³>0

  # tuning
  s::Float32                = .02;     @assert s>0
  prob_synapse::Float32     = .5;      @assert 0<=prob_synapse<=1
  Î¸_permanence01::Float32   = .5;      @assert 0<=Î¸_permanence01<=1
  pâº_01::Float32            = .1;      @assert 0<=pâº_01<=1
  pâ»_01::Float32            = .02;     @assert 0<=pâ»_01<=1
  Î¸_permanence::ğ•Šğ•¢        = round(ğ•Šğ•¢, Î¸_permanence01*typemax(ğ•Šğ•¢))
  pâº::ğ•Šğ•¢                  = round(ğ•Šğ•¢, pâº_01*typemax(ğ•Šğ•¢))
  pâ»::ğ•Šğ•¢                  = round(ğ•Šğ•¢, pâ»_01*typemax(ğ•Šğ•¢))
  Î¸_stimulus_activate::Int  = 1;       @assert Î¸_stimulus_activate>=0
  Tboost::Float32           = 200;     @assert Tboost>0
  Î²::Float32                = 1;       @assert Î²>0

  # feature gates
  enable_local_inhibit::Bool= true
  enable_learning::Bool     = true
  enable_boosting::Bool     = true
  @assert zero(ğ•Šğ•¢)<=Î¸_permanence<=typemax(ğ•Šğ•¢)
  @assert zero(ğ•Šğ•¢)<=pâº<=typemax(ğ•Šğ•¢)
  @assert zero(ğ•Šğ•¢)<=pâ»<=typemax(ğ•Šğ•¢)
end

"""
`TMParams` holds the algorithm parameters for a Temporal Memory with nomenclature
similar to [source]()

## Dimensions
- `Nc`: number of columns
- `k`: cells per column
- `Nâ‚™`: ``= k \\mathit{Nc}`` neurons in layer

## Tuning

"""
@with_kw struct TMParams
  # dimensions
  Nc::Int                  = 4096;    @assert Nc>0
  k::Int                   = 16;      @assert k>0
  Nâ‚™::Int                  = k*Nc;    @assert Nâ‚™>0

  # tuning
  pâº_01::Float32           = .12;     @assert 0<=pâº_01<=1
  pâ»_01::Float32           = .04;     @assert 0<=pâ»_01<=1
  LTD_pâ»_01::Float32       = .002;    @assert 0<=LTD_pâ»_01<=1
  pâº::ğ•Šğ•¢                 = round(ğ•Šğ•¢,pâº_01*typemax(ğ•Šğ•¢))
  pâ»::ğ•Šğ•¢                 = round(ğ•Šğ•¢,pâ»_01*typemax(ğ•Šğ•¢))
  LTD_pâ»::ğ•Šğ•¢             = round(ğ•Šğ•¢,LTD_pâ»_01*typemax(ğ•Šğ•¢))
  Î¸_permanence::ğ•Šğ•¢       = round(ğ•Šğ•¢,.5typemax(ğ•Šğ•¢))
  init_permanence::ğ•Šğ•¢    = round(ğ•Šğ•¢,.4typemax(ğ•Šğ•¢))
  synapseSampleSize::Int   = 25;      @assert synapseSampleSize>0
  Î¸_stimulus_activate::Int = 14;      @assert Î¸_stimulus_activate>0
  Î¸_stimulus_learn::Int    = 12;      @assert Î¸_stimulus_learn>0

  # feature gates
  enable_learning::Bool    = true
  @assert Î¸_stimulus_learn <= Î¸_stimulus_activate
end
