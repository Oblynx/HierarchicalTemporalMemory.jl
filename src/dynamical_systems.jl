# ## Proximal Synapses

"""
ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection} are the feedforward connections between
2 neuron layers, which can activate neurons and cause them to fire.

Used in the context of the [`SpatialPooler`](@ref).

# Description

The neurons of both layers are expected to form minicolumns which share the same feedforward connections.
The synapses are *binary*: they don't have a scalar weight, but either conduct (1) or not (0).
Instead, they have a *permanence* value D‚Çö ‚àà (0,1] and a connection threshold Œ∏.

## Initialization

Let presynaptic (input) neuron `x·µ¢` and postsynaptic (output) neuron `y·µ¢`, and a topological I/O mapping
`x·µ¢(y·µ¢) :=` [`Hypercube`](@ref)`(y·µ¢)`.
‚àÄ

## Synapse adaptation

They adapt with a hebbian learning rule.
The adaptation has a causal and an anticausal component:

- If the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened
- If the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened

The synaptic permanences are clipped at the boundaries of 0 and 1.

A simple implementation of the learning rule would look like this, where
z: input, a: output
```julia; results= "hidden"
learn!(D‚Çö,z,a)= begin
  D‚Çö[z,a]  .= (D‚Çö[z,a].>0) .* (D‚Çö[z,a]   .‚äï p‚Å∫)
  D‚Çö[.!z,a].= (D‚Çö[z,a].>0) .* (D‚Çö[.!z,a] .‚äñ p‚Åª)
end
```

# Type parameters

They allow a dense or sparse matrix representation of the synapses

- `SynapseT`: `DenseSynapses` or `SparseSynapses`
- `ConnectedT`: `DenseConnection` or `SparseConnection`

See also: [`DistalSynapses`](@ref), [`SpatialPooler`](@ref), [`TemporalMemory`](@ref)
"""
struct ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection}
  D‚Çö::SynapseT
  connected::ConnectedT

  """
  `ProximalSynapses(sz·µ¢‚Çô,sz‚Çõ‚Çö,synapseSparsity,Œ≥, prob_synapse,Œ∏_permanence)` makes an `{sz·µ¢‚Çô √ó sz‚Çõ‚Çö}` synapse permanence matrix
  and initializes potential synapses.

  # Algorithm

  For every output minicolumn `y·µ¢`:
  - find its center in the input space `x·∂ú`
  - for every input `x·µ¢ ‚àà Hypercube(x·∂ú,Œ≥)``, draw rand `Z`
    - If `Z > prob_synapse`
      - Init permanence: rescale Z from `[0..1-Œ∏] -> [0..1]: Z/(1-Œ∏)``
  """
  function ProximalSynapses(sz·µ¢‚Çô,sz‚Çõ‚Çö,synapseSparsity,Œ≥,
        prob_synapse,Œ∏_permanence)
    # Map column coordinates to their center in the input space. Column coords FROM 1 !!!
    x·∂ú(y·µ¢)= floor.(Int, (y·µ¢.-1) .* (sz·µ¢‚Çô./sz‚Çõ‚Çö)) .+1
    x·µ¢(x·∂ú)= Hypercube(x·∂ú,Œ≥,sz·µ¢‚Çô)
    Œ∏_effective()= floor(ùïäùï¢, prob_synapse*typemax(ùïäùï¢))
    out_lattice()= (c.I for c in CartesianIndices(sz‚Çõ‚Çö))

    # Draw permanences from uniform distribution. Connections aren't very sparse (40%),
    #   so prefer a dense matrix
    permanences(::Type{SparseSynapses},x·µ¢)= sprand(ùïäùï¢,length(x·µ¢),1, prob_synapse)
    permanences(::Type{DenseSynapses}, x·µ¢)= begin
      # Decide randomly if y·µ¢ ‚ü∑ x·µ¢ will connect
      p= rand(ùïäùï¢range,length(x·µ¢))
      p0= p .> Œ∏_effective(); pScale= p .< Œ∏_effective()
      fill!(view(p,p0), ùïäùï¢(0))
      # Draw permanences from uniform distribution in ùïäùï¢
      rand!(view(p,pScale), ùïäùï¢range)
      return p
    end
    fillin_permanences()= begin
      D‚Çö= zeros(ùïäùï¢, prod(sz·µ¢‚Çô),prod(sz‚Çõ‚Çö))
      foreach(out_lattice()) do y·µ¢
        # Linear indices from hypercube
        x= @>> y·µ¢ x·∂ú x·µ¢ collect map(x->c2l·µ¢‚Çô[x...])
        D‚Çö[x, c2l‚Çõ‚Çö[y·µ¢...]]= permanences(SynapseT, @> y·µ¢ x·∂ú x·µ¢)
      end
      return D‚Çö
    end
    c2l·µ¢‚Çô= LinearIndices(sz·µ¢‚Çô)
    c2l‚Çõ‚Çö= LinearIndices(sz‚Çõ‚Çö)

    SynapseT= synapseSparsity<0.05 ? SparseSynapses : DenseSynapses
    ConnectedT= synapseSparsity<0.05 ? SparseMatrixCSC{Bool} : Matrix{Bool}
    D‚Çö= fillin_permanences()
    new{SynapseT,ConnectedT}(D‚Çö, D‚Çö .> Œ∏_permanence)
  end
end
W‚Çö(s::ProximalSynapses)= s.connected

"""
`step!(s::ProximalSynapses, z,a, params)` adapts the proximal synapses' permanences with a hebbian learning rule on input `z`
and activation `a`. The adaptation has a causal and an anticausal component:

- If the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened
- If the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened

See alse: [`ProximalSynapses`](@ref)
"""
step!(s::ProximalSynapses, z,a, params)= adapt!(s.D‚Çö, s, z|> vec, a|> vec, params)

# These are performance optimizations of the simple update methods described in the ProximalSynapses doc
# - minimize allocations and accesses
function adapt!(::DenseSynapses,s::ProximalSynapses, z,a, params)
  @unpack p‚Å∫,p‚Åª,Œ∏_permanence = params
  D‚Çöactive= @view s.D‚Çö[:,a]
  activeConn=   (D‚Çöactive .> 0) .&   z
  inactiveConn= (D‚Çöactive .> 0) .& .!z
  # Learn synapse permanences according to Hebbian learning rule
  adapt_synapses!(D‚Çöactive, activeConn, inactiveConn, p‚Å∫,p‚Åª)
  # Update cache of connected synapses
  @inbounds s.connected[:,vec(a)].= D‚Çöactive .> Œ∏_permanence
end
function adapt!(::SparseSynapses,s::ProximalSynapses, z,a, params)
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,input_i)->
                    adapt_sparsesynapses!(scol,input_i, z,params.p‚Å∫,params.p‚Åª),
                 s.D‚Çö, a)
  # Update cache of connected synapses
  @inbounds s.connected[:,a].= s.D‚Çö[:,a] .> params.Œ∏_permanence
end

"""
`adapt_sparsesynapses!(synapses_activeCol,input_i,z,p‚Å∫,p‚Åª)` updates the permanence of the given vector of synapses,
which is typically a `@view` into the nonzero elements that represent an active column of the sparse array of synapses.
The input
"""
function adapt_sparsesynapses!(synapses_activeCol,input_i, z,p‚Å∫,p‚Åª)
  @inbounds z_i= z[input_i]
  adapt_synapses!(synapses_activeCol, z_i, .!z_i, p‚Å∫,p‚Åª)
end

adapt_synapses!(synapses, activeConn, inactiveConn, p‚Å∫,p‚Åª)= (
  @inbounds synapses.= activeConn .* (synapses .‚äï p‚Å∫) .+
                     inactiveConn .* (synapses .‚äñ p‚Åª)
)



# ## Distal synapses for the TM
# (assume the same learning rule governs all types of distal synapses)

"""
`DistalSynapses` are lateral connections within a neuron layer that attach to the dendrites of neurons,
not directly to the soma (neuron's center), and can therefore **depolarize** neurons but *can't activate them.*
Compare with [`ProximalSynapses`](@ref).

Used in the context of the [`TemporalMemory`](@ref).

# Description

### Synapses

Neurons have multiple signal integration zones: the soma, proximal dendrites, apical dendrites.
Signals are routed to the proximal dendrites through *distal synapses*.
This type defines both the synapses themselves and the neuron's dendrites.
The synapses themselves, like the [`ProximalSynapses`](@ref), are binary connections without weights.
They have a **permanence value**, and above a threshold they are connected.

The synapses can be represented as an adjacency matrix of dimensions `N‚Çô √ó N‚Çõ`:
presynaptic neurons -> postsynaptic dendritic segments.
This matrix is derived from the synapse permanence matrix ``D_d ‚àà \\mathit{ùïäùï¢}^{N‚Çô √ó N‚Çõ}``,
which is **sparse** (eg 0.5% synapses).
This affects the implementation of all low-level operations.

### Dendritic segments

Neurons have multiple dendritic segments carrying the distal synapses, each sufficient to depolarize the neuron (make it predictive).
The neuron/segment adjacency matrix `neurSeg` (aka `NS`) also has dimensions `N‚Çô √ó N‚Çõ`.

## Learning

Instead of being randomly initialized like the proximal synapses, distal synapses and dendrite segments are grown on demand:
when minicolumns can't predict their activation and burst, they trigger a growth process.

The synaptic permanence itself adapts similar to the proximal synapses: synapses that correctly predict are increased,
synapses that incorrectly predict are decreased.
However it's a bit more complicated to define the successful distal syapses than the proximal synapses.
"Winning segments" `WS` will adapt their synapses towards "winning neurons" `WN`.
Since synapses are considered directional, neurons are always presynaptic and segments postsynaptic.

Winning segments are those that were predicted and then activated.
Also, for every bursting minicolumn the dendrite that best "matches" the input will become winner.
If there is no sufficiently matching dendrite, a new onw will grow on the neuron that has the fewest.

Winning neurons are again those that were predicted and then activated.
Among the bursting minicolumns, the neurons bearing the winning segments are the winners.
Both definitions of winners are aligned with establishing a causal relationship: prediction -> activation.

New synapses grow from `WS` towards a random sample of `WN` at every step.
Strongly matching segments have a lower chance to grow new synapses than weakly matching segments.

!!! note
    Adding new synapses at random indices of `Dd::SparseMatrixCSC` is a performance bottleneck, because it involves
    moving the matrix's existing data. An implementation of `SparseMatrixCSC` with hashmaps could help,
    such as [SimpleSparseArrays.jl](https://github.com/jw3126/SimpleSparseArrays.jl/)

## Caching

The state of the DistalSynapses is determined by the 2 matrices ``D_d, \\mathit{NS}``.
A few extra matrices are filled in over the evolution of the distal synapses to accelerate the computations:
- `connected` caches `Dd > Œ∏_permanence`
- `segCol` caches the segment - column map (aka `SC`)
"""
mutable struct DistalSynapses
  # synapse permanence
  Dd::SparseSynapses                     # Nn √ó Nseg
  # neurons - segments
  neurSeg::SparseMatrixCSC{Bool,Int}     # Nn √ó Nseg
  # caches
  connected::SparseMatrixCSC{Bool,Int}   # Nn √ó Nsed
  segCol::SparseMatrixCSC{Bool,Int}      # Nseg √ó Ncol
  k::Int
  params::DistalSynapseParams
end
# friendly names for the matrices
NS(s::DistalSynapses)= s.neurSeg
SC(s::DistalSynapses)= s.segCol
Wd(s::DistalSynapses)= s.connected
N‚Çõ(s::DistalSynapses)= size(s.neurSeg,2)

# segments belonging to columns
col2seg(s::DistalSynapses,col::Int)= SC(s)[:,col].nzind
col2seg(s::DistalSynapses,col)= rowvals(SC(s)[:,col])
# neurons belonging to column
col2cell(col,k)= (col-1)*k+1 : col*k
# column for each cell
cell2col(cells,k)= @. (cells-1) √∑ k + 1

# Adapt distal synapses based on TM state at t-1 and current cell/column activity
# A: [Ncell] cell activity
# B: [Ncol] column activity
# WC: [Ncell] current winner cells from predicted columns; add from bursting columns
function step!(s::DistalSynapses, pWN,WS, Œ±, pŒ±,pM‚Çõ,povp_M‚Çõ)
  @unpack p‚Å∫,p‚Åª,LTD_p‚Åª,Œ∏_permanence = s.params
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,cell_i)->
                    adapt_sparsesynapses!(scol,cell_i, pŒ±, p‚Å∫,p‚Åª),
                 s.Dd, WS)
  # Decay "matching" synapses that didn't result in an active neuron
  sparse_foreach((scol,cell_i)->
                    adapt_sparsesynapses!(scol,cell_i, .!pŒ±,zero(ùïäùï¢),LTD_p‚Åª),
                 s.Dd, decayS(s,pM‚Çõ,Œ±))
  growsynapses!(s, pWN,WS, povp_M‚Çõ)
  # Update cache of connected synapses
  #@inbounds s.connected[:,WS].= s.synapses[:,WS] .> params.Œ∏_permanence
  s.connected= s.Dd .> Œ∏_permanence
end


"""
`decayS(s::DistalSynapses,pM‚Çõ,Œ±)` are the dendritic segments that should decay according to LTD (long term depression).
It's the segments that at the previous moment had enough potential synapses with active neurons to "match" the input (`pM‚Çõ`),
but didn't contribute to their neuron firing at this moment (they didn't activate strongly enough to depolarize it).
"""
decayS(s::DistalSynapses,pM‚Çõ,Œ±)= (@> pM‚Çõ padfalse(N‚Çõ(s))) .& (NS(s)'*(.!Œ±))

"""
`calculate_WS!(pŒ†‚Çõ,povp_M‚Çõ, Œ±,B)` finds the winning segments, growing new ones where necessary.
"""
function calculate_WS!(s::DistalSynapses, pŒ†‚Çõ,povp_M‚Çõ, Œ±,B)
  WS_pred= WS_predictedcol(s,pŒ†‚Çõ,Œ±)
  WS_burst= WS_burstcol!(s,B,povp_M‚Çõ)
  WS= (@> WS_pred padfalse(N‚Çõ(s))) .| WS_burst
  return (WS, WS_burst)
end

## Calculate the winning segments WS

# If a cell was previously depolarized and now becomes active, the segments that caused
# the depolarization are winning.
WS_predictedcol(s::DistalSynapses,pŒ†‚Çõ,Œ±)= pŒ†‚Çõ .& (NS(s)'Œ± .>0)
# If a cell is part of a bursting column, the segment in the column with the highest
# overlap wins.
# Foreach bursting col, find the best matching segment or grow one if needed
WS_burstcol!(s::DistalSynapses,B, povp_M‚Çõ)= begin
  burstingCols= findall(B)
  maxsegs= Vector{Option{Int}}(undef,length(burstingCols))
  map!(col->bestmatch(s,col,povp_M‚Çõ), maxsegs, burstingCols)
  growseg!(s, maxsegs, burstingCols)
  @> maxsegs bitarray(N‚Çõ(s))
end

# Best matching segment for column - or `nothing`
"""
`bestmatch(s::DistalSynapses,col, povp_M‚Çõ)` finds the best-matching segment in a column,
as long as its overlap with the input (its activation) is > Œ∏_stimulus_learn.
Otherwise, returns `nothing`.
"""
function bestmatch(s::DistalSynapses,col, povp_M‚Çõ)
  segs= col2seg(s,col)
  isempty(segs) && return nothing   # If there's no existing segments in the column
  m,i= findmax(povp_M‚Çõ[segs])
  m > s.params.Œ∏_stimulus_learn ? segs[i] : nothing
end

"""
`growsynapses!(s::DistalSynapses, pWN::CellActivity,WS, povp_M‚Çõ)` adds synapses between winning dendrites (`WS`)
and a random sample of previously winning neurons (`pWN`).

For each dendrite, target neurons are selected among `pWN` with probability to pick each neuron
determined by ``\\frac{ (\\mathit{synapseSampleSize} - \\mathit{prev_Overlap} }{ length(\\mathit{pWN}) }``
(Bernoulli sampling)

This involves inserting new elements in random places of a `SparseMatrixCSC` and is the algorithm's performance bottleneck.
- TODO: try replacing CSC with a Dict implementation of sparse matrices.
"""
function growsynapses!(s::DistalSynapses, pWN::CellActivity,WS, povp_M‚Çõ)
  @unpack synapseSampleSize, init_permanence = s.params
  _growsynapses!(pWN)= begin
    Nnewsyn(ovp)= max(0,synapseSampleSize - ovp)
    # probability to sample each new synapse
    psampling_newsyn= min.(1.0, Nnewsyn.(padfalse(povp_M‚Çõ,N‚Çõ(s))[WS]) ./ length(pWN))
    # preallocate storage for sampling
    selectedWN= similar(pWN)
    foreach(Truesof(WS), psampling_newsyn) do seg_i, p
      # Bernoulli sampling from WN with mean sample size == Nnewsyn
      randsubseq!(selectedWN,pWN,p)
      # Grow new synapses (percolumn manual | setindex percolumn | setindex WN,WS,sparse_V)
      s.Dd[selectedWN, seg_i].= init_permanence
    end
  end

  pWN= findall(pWN)
  !isempty(pWN) && _growsynapses!(pWN)
end

"""
`leastusedcell(s::DistalSynapses,col)` finds the neuron of a minicolumn with the fewest dendrites.
- TODO: try Memoize.jl !
"""
function leastusedcell(s::DistalSynapses,col)
  neuronsWithSegs= NS(s)[:,col2seg(s,col)].rowval|> countmap_empty
  neuronsWithoutSegs= setdiff(col2cell(col,s.k), neuronsWithSegs|> keys)
  # If there's no neurons without dendrites, return the one with the fewest
  isempty(neuronsWithoutSegs) ?
      findmin(neuronsWithSegs)[2] : rand(neuronsWithoutSegs)
end

# OPTIMIZE add many segments together for efficiency?
"""
`growseg!(s::DistalSynapses,WS_col::Vector{Option{Int}},burstingcolidx)` grows 1 new segment for each column with `nothing`
as winning segment and replaces `nothing` with it. It resizes all the `DistalSynapses` matrices to append
new dendritic segments. Foreach bursting minicolumn without a winning segment,
the neuron to grow the segment is the one with the fewest segments.
"""
function growseg!(s::DistalSynapses,WS_col::Vector{Option{Int}},burstingcolidx)
  neuronsToGrow= map(col-> leastusedcell(s,col), burstingcolidx[isnothing.(WS_col)])
  columnsToGrow= cell2col(neuronsToGrow,s.k)
  Ncell= size(s.Dd,1); Ncol= size(s.segCol,2)
  Nseg_before= N‚Çõ(s)
  Nseggrow= length(neuronsToGrow)  # grow 1 seg per cell
  _grow_synapse_matrices!(s,columnsToGrow,neuronsToGrow,Nseggrow)
  # Replace in maxsegs, `nothing` with something :-)
  WS_col[isnothing.(WS_col)].= Nseg_before.+(1:Nseggrow)
end
function _grow_synapse_matrices!(s::DistalSynapses,columnsToGrow,neuronsToGrow,Nseggrow)
  s.Dd= hcat!!(s.Dd, Nseggrow)
  s.neurSeg= hcat!!(s.neurSeg, neuronsToGrow, trues(Nseggrow))
  s.connected= hcat!!(s.connected, neuronsToGrow, falses(Nseggrow))
  s.segCol= vcat!!(s.segCol, columnsToGrow, trues(Nseggrow))
end

"""
`countmap_empty(x)` [Dict] Count the frequency of occurence for each element in x
"""
countmap_empty(x)= isempty(x) ? x : countmap(x)