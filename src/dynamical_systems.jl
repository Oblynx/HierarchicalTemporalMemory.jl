# ## Inhibition Radius

"""
The inhibition radius of a Spatial Pooler's columns is a dynamical system that evolves
under the influence of other elements of the Spatial Pooler. It provides an init
(constructor) and a step! function.
"""
mutable struct InhibitionRadius <:AbstractFloat
  œÜ::Float32
  InhibitionRadius(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö,enable_local_inhibit=true)=
      enable_local_inhibit ?
        new(simplified_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö)) :
        new(maximum(sz‚Çõ‚Çö)+1)
end

function step!(s::InhibitionRadius, params)
  @unpack Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö,enable_local_inhibit = params
  if enable_local_inhibit
    s.œÜ= simplified_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö)
  end
end

simplified_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö)= begin
  mean_receptiveFieldSpan()= (Œ≥*2+0.5)*prob_synapse
  receptiveFieldSpan_yspace()= (mean_receptiveFieldSpan()*mean(sz‚Çõ‚Çö./sz·µ¢‚Çô)-1)/2
  max(receptiveFieldSpan_yspace(), 1)
end

# This implementation follows the SP paper description and NUPIC, but seems too complex
#   for no reason. Replace with a static inhibition radius instead
#nupic_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö,s::InhibitionRadius)= begin
#  mean_receptiveFieldSpan()::Float32= mapslices(receptiveFieldSpan, W, dims=1)|> mean
#  receptiveFieldSpan_yspace()= (mean_receptiveFieldSpan()*mean(sz‚Çõ‚Çö./sz·µ¢‚Çô)-1)/2
#  max(receptiveFieldSpan_yspace(), 1)
#end
#receptiveFieldSpan(colinputs,s::InhibitionRadius)::Float32= begin
#  connectedInputCoords= @>> colinputs findall getindex(s.ci)
#  maxc= [mapreduce(c->c.I[d], max, connectedInputCoords) for d in 1:Nin]
#  minc= [mapreduce(c->c.I[d], min, connectedInputCoords) for d in 1:Nin]
#  mean(maxc .- minc .+ 1)
#end


# ## Proximal Synapses

"""
ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection} are the feedforward connections between
2 neuron layers, which can activate neurons and cause them to fire.

Used in the context of the [`SpatialPooler`](@ref).

# Description

The neurons of both layers are expected to form minicolumns which share the same feedforward connections.
The synapses are *binary*: they don't have a scalar weight, but either conduct (1) or not (0).
Instead, they have a *permanence* value D‚Çö ‚àà (0,1] and a connection threshold Œ∏.

## Initialization

Let presynaptic (input) neuron `x·µ¢` and postsynaptic (output) neuron `y·µ¢`, and a topological I/O mapping
`x·µ¢(y·µ¢) :=` [`Hypercube`](@ref)`(y·µ¢)`.
‚àÄ

## Synapse adaptation

They adapt with a hebbian learning rule.
The adaptation has a causal and an anticausal component:

- If the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened
- If the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened

The synaptic permanences are clipped at the boundaries of 0 and 1.

A simple implementation of the learning rule would look like this, where
z: input, a: output
```julia; results= "hidden"
learn!(D‚Çö,z,a)= begin
  D‚Çö[z,a]  .= (D‚Çö[z,a].>0) .* (D‚Çö[z,a]   .‚äï p‚Å∫)
  D‚Çö[.!z,a].= (D‚Çö[z,a].>0) .* (D‚Çö[.!z,a] .‚äñ p‚Åª)
end
```

# Type parameters

They allow a dense or sparse matrix representation of the synapses

- `SynapseT`: `DenseSynapses` or `SparseSynapses`
- `ConnectedT`: `DenseConnection` or `SparseConnection`

See also: [`DistalSynapses`](@ref), [`SpatialPooler`](@ref), [`TemporalMemory`](@ref)
"""
struct ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection}
  D‚Çö::SynapseT
  connected::ConnectedT

  """
  `ProximalSynapses(sz·µ¢‚Çô,sz‚Çõ‚Çö,synapseSparsity,Œ≥, prob_synapse,Œ∏_permanence)` makes an `{sz·µ¢‚Çô √ó sz‚Çõ‚Çö}` synapse permanence matrix
  and initializes potential synapses.

  # Algorithm

  For every output minicolumn `y·µ¢`:
  - find its center in the input space `x·∂ú`
  - for every input `x·µ¢ ‚àà Hypercube(x·∂ú,Œ≥)``, draw rand `Z`
    - If `Z > prob_synapse`
      - Init permanence: rescale Z from `[0..1-Œ∏] -> [0..1]: Z/(1-Œ∏)``
  """
  function ProximalSynapses(sz·µ¢‚Çô,sz‚Çõ‚Çö,synapseSparsity,Œ≥,
        prob_synapse,Œ∏_permanence)
    # Map column coordinates to their center in the input space. Column coords FROM 1 !!!
    x·∂ú(y·µ¢)= floor.(Int, (y·µ¢.-1) .* (sz·µ¢‚Çô./sz‚Çõ‚Çö)) .+1
    x·µ¢(x·∂ú)= Hypercube(x·∂ú,Œ≥,sz·µ¢‚Çô)
    Œ∏_effective()= floor(ùïäùï¢, prob_synapse*typemax(ùïäùï¢))
    out_lattice()= (c.I for c in CartesianIndices(sz‚Çõ‚Çö))

    # Draw permanences from uniform distribution. Connections aren't very sparse (40%),
    #   so prefer a dense matrix
    permanences(::Type{SparseSynapses},x·µ¢)= sprand(ùïäùï¢,length(x·µ¢),1, prob_synapse)
    permanences(::Type{DenseSynapses}, x·µ¢)= begin
      # Decide randomly if y·µ¢ ‚ü∑ x·µ¢ will connect
      p= rand(ùïäùï¢range,length(x·µ¢))
      p0= p .> Œ∏_effective(); pScale= p .< Œ∏_effective()
      fill!(view(p,p0), ùïäùï¢(0))
      # Draw permanences from uniform distribution in ùïäùï¢
      rand!(view(p,pScale), ùïäùï¢range)
      return p
    end
    fillin_permanences()= begin
      D‚Çö= zeros(ùïäùï¢, prod(sz·µ¢‚Çô),prod(sz‚Çõ‚Çö))
      foreach(out_lattice()) do y·µ¢
        # Linear indices from hypercube
        x= @>> y·µ¢ x·∂ú x·µ¢ collect map(x->c2l·µ¢‚Çô[x...])
        D‚Çö[x, c2l‚Çõ‚Çö[y·µ¢...]]= permanences(SynapseT, @> y·µ¢ x·∂ú x·µ¢)
      end
      return D‚Çö
    end
    c2l·µ¢‚Çô= LinearIndices(sz·µ¢‚Çô)
    c2l‚Çõ‚Çö= LinearIndices(sz‚Çõ‚Çö)

    SynapseT= synapseSparsity<0.05 ? SparseSynapses : DenseSynapses
    ConnectedT= synapseSparsity<0.05 ? SparseMatrixCSC{Bool} : Matrix{Bool}
    D‚Çö= fillin_permanences()
    new{SynapseT,ConnectedT}(D‚Çö, D‚Çö .> Œ∏_permanence)
  end
end
W‚Çö(s::ProximalSynapses)= s.connected

"""
`step!(s::ProximalSynapses, z,a, params)` adapts the proximal synapses' permanences with a hebbian learning rule on input `z`
and activation `a`. The adaptation has a causal and an anticausal component:

- If the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened
- If the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened

See alse: [`ProximalSynapses`](@ref)
"""
step!(s::ProximalSynapses, z,a, params)= adapt!(s.D‚Çö, s,z,a,params)

# These are performance optimizations of the simple update methods described in the ProximalSynapses doc
# - minimize allocations and accesses
function adapt!(::DenseSynapses,s::ProximalSynapses, z,a, params)
  @unpack p‚Å∫,p‚Åª,Œ∏_permanence = params
  D‚Çöactive= @view s.D‚Çö[:,a]
  activeConn=   (D‚Çöactive .> 0) .&   z
  inactiveConn= (D‚Çöactive .> 0) .& .!z
  # Learn synapse permanences according to Hebbian learning rule
  @inbounds D‚Çöactive.= activeConn   .* (D‚Çöactive .‚äï p‚Å∫) .+
                       inactiveConn .* (D‚Çöactive .‚äñ p‚Åª)
  # Update cache of connected synapses
  @inbounds s.connected[:,vec(a)].= D‚Çöactive .> Œ∏_permanence
end
function adapt!(::SparseSynapses,s::ProximalSynapses, z,a, params)
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,input_i)->
                    learn_sparsesynapses!(scol,input_i, z,params.p‚Å∫,params.p‚Åª),
                 s.D‚Çö, a)
  # Update cache of connected synapses
  @inbounds s.connected[:,a].= s.D‚Çö[:,a] .> params.Œ∏_permanence
end
function learn_sparsesynapses!(synapses_activeCol,input_i,z,p‚Å∫,p‚Åª)
  @inbounds z_i= z[input_i]
  @inbounds synapses_activeCol.= z_i .* (synapses_activeCol .‚äï p‚Å∫) .+
                               .!z_i .* (synapses_activeCol .‚äñ p‚Åª)
end



# ## Distal synapses for the TM
# (assume the same learning rule governs all types of distal synapses)

"""
`DistalSynapses` are lateral connections within a neuron layer that attach to the dendrites of neurons,
not directly to the soma (neuron's center), and can therefore **depolarize** neurons but *can't activate them.*
Compare with [`ProximalSynapses`](@ref).

Used in the context of the [`TemporalMemory`](@ref).

# Description

Neurons have multiple signal integration zones: the soma, proximal dendrites, apical dendrites.
Signals are routed to the proximal dendrites through distal synapses.
This type defines both the synapses themselves and the neuron's dendrites.
"""
mutable struct DistalSynapses
  synapses::SparseSynapses               # Ncell x Nseg
  connected::SparseMatrixCSC{Bool,Int}
  cellSeg::SparseMatrixCSC{Bool,Int}     # Ncell x Nseg
  segCol::SparseMatrixCSC{Bool,Int}      # Nseg x Ncol
  cellœµcol::Int
end
cellXseg(s::DistalSynapses)= s.cellSeg
col2seg(s::DistalSynapses,col::Int)= s.segCol[:,col].nzind
col2seg(s::DistalSynapses,col)= rowvals(s.segCol[:,col])
col2cell(col,cellœµcol)= (col-1)*cellœµcol+1 : col*cellœµcol
cell2col(cells,cellœµcol)= @. (cells-1) √∑ cellœµcol + 1
connected(s::DistalSynapses)= s.connected

# Adapt distal synapses based on TM state at t-1 and current cell/column activity
# A: [Ncell] cell activity
# B: [Ncol] column activity
# WC: [Ncell] current winner cells from predicted columns; add from bursting columns
function step!(s::DistalSynapses,WC,previous::NamedTuple,A,B, params)
  WS= get_grow__winseg_wincell!(s,WC, previous.Œ†‚Çõ,A, B,previous.ovp_M‚Çõ,params.Œ∏_stimulus_learn)
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, previous.A,params.p‚Å∫,params.p‚Åª),
                 s.synapses, WS)
  # Decay unused synapses
  decayS= padfalse(previous.M‚Çõ,length(WS)) .& (s.cellSeg'*(.!A))
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, .!previous.A,zero(ùïäùï¢),params.LTD_p‚Åª),
                 s.synapses, decayS)
  growsynapses!(s, previous.WC,WS, previous.ovp_M‚Çõ,params.synapseSampleSize,params.init_permanence)
  # Update cache of connected synapses
  #@inbounds s.connected[:,WS].= s.synapses[:,WS] .> params.Œ∏_permanence
  s.connected= s.synapses .> params.Œ∏_permanence_dist
end
# Calculate and return WinningSegments, growing new segments where needed.
#   Update WinnerCells with bursting columns.
function get_grow__winseg_wincell!(s,WC, Œ†‚Çõ,A, B,ovp_M‚Çõ,Œ∏_stimulus_learn)
  WS_activecol= winningSegments_activecol(s,Œ†‚Çõ,A)
  WS_burstcol= maxsegœµburstcol!(s,B,ovp_M‚Çõ,Œ∏_stimulus_learn)
  Nseg= size(s.cellSeg,2)
  WS= (@> WS_activecol padfalse(Nseg)) .| WS_burstcol
  # Update winner cells with entries from bursting columns
  WC[cellXseg(s)*WS_burstcol.>0].= true
  return WS
end
growsynapses!(s, WC::CellActivity,WS, ovp_M‚Çõ,synapseSampleSize,init_permanence)= begin
  WC= findall(WC)
  !isempty(WC) && _growsynapses!(s, WC,WS, ovp_M‚Çõ,synapseSampleSize,init_permanence)
end
function _growsynapses!(s, WC,WS, ovp_M‚Çõ,synapseSampleSize,init_permanence)
  Nnewsyn(ovp)= max(0,synapseSampleSize - ovp)
  Nseg= size(s.cellSeg,2)
  psampling_newsyn= min.(1.0, Nnewsyn.((@> ovp_M‚Çõ padfalse(Nseg))[WS]) ./ length(WC))
  selectedWC= similar(WC)

  foreach(Truesof(WS), psampling_newsyn) do seg_i, p
    # Bernoulli sampling from WC with mean sample size == Nnewsyn
    randsubseq!(selectedWC,WC,p)
    # Grow new synapses (percolumn manual | setindex percolumn | setindex WC,WS,sparse_V)
    s.synapses[selectedWC, seg_i].= init_permanence
  end
end

# If a cell was previously depolarized and now becomes active, the segments that caused
# the depolarization are winning.
winningSegments_activecol(synapses,Œ†‚Çõ,A)= Œ†‚Çõ .& (cellXseg(synapses)'A .>0)
# If a cell is part of a bursting column, the segment in the column with the highest
# overlap wins.
# Foreach bursting col, find the best matching segment or grow one if needed
maxsegœµburstcol!(synapses,B,ovp_M‚Çõ,Œ∏_stimulus_learn)= begin
  burstingCols= findall(B)
  maxsegs= Vector{Option{Int}}(undef,length(burstingCols))
  map!(col->bestmatch(synapses,col,ovp_M‚Çõ,Œ∏_stimulus_learn), maxsegs, burstingCols)
  growseg!(synapses,maxsegs,burstingCols)
  Nseg= size(cellXseg(synapses),2)
  @> maxsegs bitarray(Nseg)
end

# Best matching segment for column - or `nothing`
function bestmatch(synapses,col,ovp_M‚Çõ,Œ∏_stimulus_learn)
  segs= col2seg(synapses,col)
  isempty(segs) && return nothing   # If there's no existing segments in the column
  m,i= findmax(ovp_M‚Çõ[segs])
  m > Œ∏_stimulus_learn ? segs[i] : nothing
end
# Cell with least segments
function leastusedcell(synapses,col)
  cellsWithSegs= cellXseg(synapses)[:,col2seg(synapses,col)].rowval|> countmap_empty
  cellsWithoutSegs= setdiff(col2cell(col,synapses.cellœµcol), cellsWithSegs|> keys)
  isempty(cellsWithoutSegs) ?
      findmin(cellsWithSegs)[2] : rand(cellsWithoutSegs)
end

# OPTIMIZE add many segments together for efficiency?
function growseg!(synapses::DistalSynapses,maxsegs::Vector{Option{Int}},burstingcolidx)
  cellsToGrow= map(col-> leastusedcell(synapses,col), burstingcolidx[isnothing.(maxsegs)])
  columnsToGrow= cell2col(cellsToGrow,synapses.cellœµcol)
  Ncell= size(synapses.synapses,1); Ncol= size(synapses.segCol,2)
  Nseg_before= size(cellXseg(synapses),2)
  Nseggrow= length(cellsToGrow)  # grow 1 seg per cell
  _grow_synapse_matrices!(synapses,columnsToGrow,cellsToGrow,Nseggrow)
  # Replace in maxsegs, `nothing` with something :-)
  maxsegs[isnothing.(maxsegs)].= Nseg_before.+(1:Nseggrow)
end
function _grow_synapse_matrices!(synapses::DistalSynapses,columnsToGrow,cellsToGrow,Nseggrow)
  synapses.segCol= vcat!!(synapses.segCol, columnsToGrow, trues(Nseggrow))
  synapses.cellSeg= hcat!!(synapses.cellSeg, cellsToGrow,trues(Nseggrow))
  synapses.connected= hcat!!(synapses.connected, cellsToGrow,falses(Nseggrow))
  synapses.synapses= hcat!!(synapses.synapses, Nseggrow)
end

# [Dict] Count the frequency of occurence for each element in x
countmap_empty(x)= isempty(x) ? x : countmap(x)
