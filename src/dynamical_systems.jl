# ## Inhibition Radius

"""
The inhibition radius of a Spatial Pooler's columns is a dynamical system that evolves
under the influence of other elements of the Spatial Pooler. It provides an init
(constructor) and a step! function.
"""
mutable struct InhibitionRadius <:AbstractFloat
  œÜ::Float32
  InhibitionRadius(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö,enable_local_inhibit=true)=
      enable_local_inhibit ?
        new(simplified_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö)) :
        new(maximum(sz‚Çõ‚Çö)+1)
end

function step!(s::InhibitionRadius, params)
  @unpack Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö,enable_local_inhibit = params
  if enable_local_inhibit
    s.œÜ= simplified_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö)
  end
end

simplified_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö)= begin
  mean_receptiveFieldSpan()= (Œ≥*2+0.5)*prob_synapse
  receptiveFieldSpan_yspace()= (mean_receptiveFieldSpan()*mean(sz‚Çõ‚Çö./sz·µ¢‚Çô)-1)/2
  max(receptiveFieldSpan_yspace(), 1)
end

# This implementation follows the SP paper description and NUPIC, but seems too complex
#   for no reason. Replace with a static inhibition radius instead
#nupic_update_œÜ(Œ≥,prob_synapse,sz·µ¢‚Çô,sz‚Çõ‚Çö,s::InhibitionRadius)= begin
#  mean_receptiveFieldSpan()::Float32= mapslices(receptiveFieldSpan, W, dims=1)|> mean
#  receptiveFieldSpan_yspace()= (mean_receptiveFieldSpan()*mean(sz‚Çõ‚Çö./sz·µ¢‚Çô)-1)/2
#  max(receptiveFieldSpan_yspace(), 1)
#end
#receptiveFieldSpan(colinputs,s::InhibitionRadius)::Float32= begin
#  connectedInputCoords= @>> colinputs findall getindex(s.ci)
#  maxc= [mapreduce(c->c.I[d], max, connectedInputCoords) for d in 1:Nin]
#  minc= [mapreduce(c->c.I[d], min, connectedInputCoords) for d in 1:Nin]
#  mean(maxc .- minc .+ 1)
#end


# ## Proximal Synapses

"""
ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection} are the feedforward connections between
2 neuron layers, which can activate neurons and cause them to fire.

Used in the context of the [`SpatialPooler`](@ref).

# Description

The neurons of both layers are expected to form minicolumns which share the same feedforward connections.
The synapses are *binary*: they don't have a scalar weight, but either conduct (1) or not (0).
Instead, they have a *permanence* value D‚Çö ‚àà (0,1] and a connection threshold Œ∏.

## Initialization

Let presynaptic (input) neuron `x·µ¢` and postsynaptic (output) neuron `y·µ¢`, and a topological I/O mapping
`x·µ¢(y·µ¢) :=` [`Hypercube`](@ref)`(y·µ¢)`.
‚àÄ

## Synapse adaptation

They adapt with a hebbian learning rule.
The adaptation has a causal and an anticausal component:

- If the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened
- If the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened

The synaptic permanences are clipped at the boundaries of 0 and 1.

A simple implementation of the learning rule would look like this, where
z: input, a: output
```julia; results= "hidden"
learn!(D‚Çö,z,a)= begin
  D‚Çö[z,a]  .= (D‚Çö[z,a].>0) .* (D‚Çö[z,a]   .‚äï p‚Å∫)
  D‚Çö[.!z,a].= (D‚Çö[z,a].>0) .* (D‚Çö[.!z,a] .‚äñ p‚Åª)
end
```

# Type parameters

They allow a dense or sparse matrix representation of the synapses

- `SynapseT`: `DenseSynapses` or `SparseSynapses`
- `ConnectedT`: `DenseConnection` or `SparseConnection`

See also: [`DistalSynapses`](@ref), [`SpatialPooler`](@ref), [`TemporalMemory`](@ref)
"""
struct ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection}
  D‚Çö::SynapseT
  connected::ConnectedT

  """
  `ProximalSynapses(sz·µ¢‚Çô,sz‚Çõ‚Çö,synapseSparsity,Œ≥, prob_synapse,Œ∏_permanence)` makes an `{sz·µ¢‚Çô √ó sz‚Çõ‚Çö}` synapse permanence matrix
  and initializes potential synapses.

  # Algorithm

  For every output minicolumn `y·µ¢`:
  - find its center in the input space `x·∂ú`
  - for every input `x·µ¢ ‚àà Hypercube(x·∂ú,Œ≥)``, draw rand `Z`
    - If `Z > prob_synapse`
      - Init permanence: rescale Z from `[0..1-Œ∏] -> [0..1]: Z/(1-Œ∏)``
  """
  function ProximalSynapses(sz·µ¢‚Çô,sz‚Çõ‚Çö,synapseSparsity,Œ≥,
        prob_synapse,Œ∏_permanence)
    # Map column coordinates to their center in the input space. Column coords FROM 1 !!!
    x·∂ú(y·µ¢)= floor.(Int, (y·µ¢.-1) .* (sz·µ¢‚Çô./sz‚Çõ‚Çö)) .+1
    x·µ¢(x·∂ú)= Hypercube(x·∂ú,Œ≥,sz·µ¢‚Çô)
    Œ∏_effective()= floor(ùïäùï¢, prob_synapse*typemax(ùïäùï¢))
    out_lattice()= (c.I for c in CartesianIndices(sz‚Çõ‚Çö))

    # Draw permanences from uniform distribution. Connections aren't very sparse (40%),
    #   so prefer a dense matrix
    permanences(::Type{SparseSynapses},x·µ¢)= sprand(ùïäùï¢,length(x·µ¢),1, prob_synapse)
    permanences(::Type{DenseSynapses}, x·µ¢)= begin
      # Decide randomly if y·µ¢ ‚ü∑ x·µ¢ will connect
      p= rand(ùïäùï¢range,length(x·µ¢))
      p0= p .> Œ∏_effective(); pScale= p .< Œ∏_effective()
      fill!(view(p,p0), ùïäùï¢(0))
      # Draw permanences from uniform distribution in ùïäùï¢
      rand!(view(p,pScale), ùïäùï¢range)
      return p
    end
    fillin_permanences()= begin
      D‚Çö= zeros(ùïäùï¢, prod(sz·µ¢‚Çô),prod(sz‚Çõ‚Çö))
      foreach(out_lattice()) do y·µ¢
        # Linear indices from hypercube
        x= @>> y·µ¢ x·∂ú x·µ¢ collect map(x->c2l·µ¢‚Çô[x...])
        D‚Çö[x, c2l‚Çõ‚Çö[y·µ¢...]]= permanences(SynapseT, @> y·µ¢ x·∂ú x·µ¢)
      end
      return D‚Çö
    end
    c2l·µ¢‚Çô= LinearIndices(sz·µ¢‚Çô)
    c2l‚Çõ‚Çö= LinearIndices(sz‚Çõ‚Çö)

    SynapseT= synapseSparsity<0.05 ? SparseSynapses : DenseSynapses
    ConnectedT= synapseSparsity<0.05 ? SparseMatrixCSC{Bool} : Matrix{Bool}
    D‚Çö= fillin_permanences()
    new{SynapseT,ConnectedT}(D‚Çö, D‚Çö .> Œ∏_permanence)
  end
end
W‚Çö(s::ProximalSynapses)= s.connected

"""
`step!(s::ProximalSynapses, z,a, params)` adapts the proximal synapses' permanences with a hebbian learning rule on input `z`
and activation `a`. The adaptation has a causal and an anticausal component:

- If the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened
- If the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened

See alse: [`ProximalSynapses`](@ref)
"""
step!(s::ProximalSynapses, z,a, params)= adapt!(s.D‚Çö, s,z,a,params)

# These are performance optimizations of the simple update methods described in the ProximalSynapses doc
# - minimize allocations and accesses
function adapt!(::DenseSynapses,s::ProximalSynapses, z,a, params)
  @unpack p‚Å∫,p‚Åª,Œ∏_permanence = params
  D‚Çöactive= @view s.D‚Çö[:,a]
  activeConn=   (D‚Çöactive .> 0) .&   z
  inactiveConn= (D‚Çöactive .> 0) .& .!z
  # Learn synapse permanences according to Hebbian learning rule
  @inbounds D‚Çöactive.= activeConn   .* (D‚Çöactive .‚äï p‚Å∫) .+
                       inactiveConn .* (D‚Çöactive .‚äñ p‚Åª)
  # Update cache of connected synapses
  @inbounds s.connected[:,vec(a)].= D‚Çöactive .> Œ∏_permanence
end
function adapt!(::SparseSynapses,s::ProximalSynapses, z,a, params)
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,input_i)->
                    learn_sparsesynapses!(scol,input_i, z,params.p‚Å∫,params.p‚Åª),
                 s.D‚Çö, a)
  # Update cache of connected synapses
  @inbounds s.connected[:,a].= s.D‚Çö[:,a] .> params.Œ∏_permanence
end
function learn_sparsesynapses!(synapses_activeCol,input_i,z,p‚Å∫,p‚Åª)
  @inbounds z_i= z[input_i]
  @inbounds synapses_activeCol.= z_i .* (synapses_activeCol .‚äï p‚Å∫) .+
                               .!z_i .* (synapses_activeCol .‚äñ p‚Åª)
end



# ## Distal synapses for the TM
# (assume the same learning rule governs all types of distal synapses)

"""
`DistalSynapses` are lateral connections within a neuron layer that attach to the dendrites of neurons,
not directly to the soma (neuron's center), and can therefore **depolarize** neurons but *can't activate them.*
Compare with [`ProximalSynapses`](@ref).

Used in the context of the [`TemporalMemory`](@ref).

# Description

### Synapses

Neurons have multiple signal integration zones: the soma, proximal dendrites, apical dendrites.
Signals are routed to the proximal dendrites through *distal synapses*.
This type defines both the synapses themselves and the neuron's dendrites.
The synapses themselves, like the [`ProximalSynapses`](@ref), are binary connections without weights.
They have a **permanence value**, and above a threshold they are connected.

The synapses can be represented as an adjacency matrix of dimensions `N‚Çô √ó N‚Çõ`:
presynaptic neurons -> postsynaptic dendritic segments.
This matrix is derived from the synapse permanence matrix ``D_d ‚àà \\mathit{ùïäùï¢}^{N‚Çô √ó N‚Çõ}``,
which is **sparse** (eg 0.5% synapses).
This affects the implementation of all low-level operations.

### Dendritic segments

Neurons have multiple dendritic segments carrying the distal synapses, each sufficient to depolarize the neuron (make it predictive).
The neuron/segment adjacency matrix `neurSeg` (aka `NS`) also has dimensions `N‚Çô √ó N‚Çõ`.

## Learning

Instead of being randomly initialized like the proximal synapses, distal synapses and dendrite segments are grown on demand:
when minicolumns can't predict their activation and burst, they trigger a growth process.

The synaptic permanence itself adapts similar to the proximal synapses: synapses that correctly predict are increased,
synapses that incorrectly predict are decreased.
However it's a bit more complicated to define the successful distal syapses than the proximal synapses.
"Winning segments" `WS` will adapt their synapses towards "winning neurons" `WN`.
Since synapses are considered directional, neurons are always presynaptic and segments postsynaptic.

Winning segments are those that were predicted and then activated.
Also, for every bursting minicolumn the dendrite that best "matches" the input will become winner.
If there is no sufficiently matching dendrite, a new onw will grow on the neuron that has the fewest.

Winning neurons are again those that were predicted and then activated.
Among the bursting minicolumns, the neurons bearing the winning segments are the winners.
Both definitions of winners are aligned with establishing a causal relationship: prediction -> activation.

New synapses grow from `WS` towards a random sample of `WN` at every step.
Strongly matching segments have a lower chance to grow new synapses than weakly matching segments.

## Caching

The state of the DistalSynapses is determined by the 2 matrices ``D_d, \\mathit{NS}``.
A few extra matrices are filled in over the evolution of the distal synapses to accelerate the computations:
- `connected` caches `Dd > Œ∏_permanence`
- `segCol` caches the segment - column map (aka `SC`)
"""
mutable struct DistalSynapses
  # synapse permanence
  Dd::SparseSynapses                     # Nn √ó Nseg
  # neurons - segments
  neurSeg::SparseMatrixCSC{Bool,Int}     # Nn √ó Nseg
  # caches
  connected::SparseMatrixCSC{Bool,Int}   # Nn √ó Nsed
  segCol::SparseMatrixCSC{Bool,Int}      # Nseg √ó Ncol
  k::Int
end
# friendly names for the matrices
NS(s::DistalSynapses)= s.neurSeg
SC(s::DistalSynapses)= s.segCol
Wd(s::DistalSynapses)= s.connected

# segments belonging to columns
col2seg(s::DistalSynapses,col::Int)= s.segCol[:,col].nzind
col2seg(s::DistalSynapses,col)= rowvals(s.segCol[:,col])
# neurons belonging to column
col2cell(col,k)= (col-1)*k+1 : col*k
# column for each cell
cell2col(cells,k)= @. (cells-1) √∑ k + 1

# Adapt distal synapses based on TM state at t-1 and current cell/column activity
# A: [Ncell] cell activity
# B: [Ncol] column activity
# WC: [Ncell] current winner cells from predicted columns; add from bursting columns
function step!(s::DistalSynapses,WC,previous::NamedTuple,A,B, params)
  WS= get_grow__winseg_wincell!(s,WC, previous.Œ†‚Çõ,A, B,previous.ovp_M‚Çõ,params.Œ∏_stimulus_learn)
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, previous.A,params.p‚Å∫,params.p‚Åª),
                 s.Dd, WS)
  # Decay unused synapses
  decayS= padfalse(previous.M‚Çõ,length(WS)) .& (s.neurSeg'*(.!A))
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, .!previous.A,zero(ùïäùï¢),params.LTD_p‚Åª),
                 s.Dd, decayS)
  growsynapses!(s, previous.WC,WS, previous.ovp_M‚Çõ,params.synapseSampleSize,params.init_permanence)
  # Update cache of connected synapses
  #@inbounds s.connected[:,WS].= s.synapses[:,WS] .> params.Œ∏_permanence
  s.connected= s.Dd .> params.Œ∏_permanence
end
# Calculate and return WinningSegments, growing new segments where needed.
#   Update WinnerCells with bursting columns.
function get_grow__winseg_wincell!(s,WC, Œ†‚Çõ,A, B,ovp_M‚Çõ,Œ∏_stimulus_learn)
  WS_activecol= winningSegments_activecol(s,Œ†‚Çõ,A)
  WS_burstcol= maxsegœµburstcol!(s,B,ovp_M‚Çõ,Œ∏_stimulus_learn)
  Nseg= size(s.neurSeg,2)
  WS= (@> WS_activecol padfalse(Nseg)) .| WS_burstcol
  # Update winner cells with entries from bursting columns
  WC[NS(s)*WS_burstcol.>0].= true
  return WS
end
growsynapses!(s, WC::CellActivity,WS, ovp_M‚Çõ,synapseSampleSize,init_permanence)= begin
  WC= findall(WC)
  !isempty(WC) && _growsynapses!(s, WC,WS, ovp_M‚Çõ,synapseSampleSize,init_permanence)
end
function _growsynapses!(s, WC,WS, ovp_M‚Çõ,synapseSampleSize,init_permanence)
  Nnewsyn(ovp)= max(0,synapseSampleSize - ovp)
  Nseg= size(s.neurSeg,2)
  psampling_newsyn= min.(1.0, Nnewsyn.((@> ovp_M‚Çõ padfalse(Nseg))[WS]) ./ length(WC))
  selectedWC= similar(WC)

  foreach(Truesof(WS), psampling_newsyn) do seg_i, p
    # Bernoulli sampling from WC with mean sample size == Nnewsyn
    randsubseq!(selectedWC,WC,p)
    # Grow new synapses (percolumn manual | setindex percolumn | setindex WC,WS,sparse_V)
    s.Dd[selectedWC, seg_i].= init_permanence
  end
end

# If a cell was previously depolarized and now becomes active, the segments that caused
# the depolarization are winning.
winningSegments_activecol(synapses,Œ†‚Çõ,A)= Œ†‚Çõ .& (NS(synapses)'A .>0)
# If a cell is part of a bursting column, the segment in the column with the highest
# overlap wins.
# Foreach bursting col, find the best matching segment or grow one if needed
maxsegœµburstcol!(synapses,B,ovp_M‚Çõ,Œ∏_stimulus_learn)= begin
  burstingCols= findall(B)
  maxsegs= Vector{Option{Int}}(undef,length(burstingCols))
  map!(col->bestmatch(synapses,col,ovp_M‚Çõ,Œ∏_stimulus_learn), maxsegs, burstingCols)
  growseg!(synapses,maxsegs,burstingCols)
  Nseg= size(NS(synapses),2)
  @> maxsegs bitarray(Nseg)
end

# Best matching segment for column - or `nothing`
function bestmatch(synapses,col,ovp_M‚Çõ,Œ∏_stimulus_learn)
  segs= col2seg(synapses,col)
  isempty(segs) && return nothing   # If there's no existing segments in the column
  m,i= findmax(ovp_M‚Çõ[segs])
  m > Œ∏_stimulus_learn ? segs[i] : nothing
end
# Cell with least segments
function leastusedcell(synapses,col)
  cellsWithSegs= NS(synapses)[:,col2seg(synapses,col)].rowval|> countmap_empty
  cellsWithoutSegs= setdiff(col2cell(col,synapses.k), cellsWithSegs|> keys)
  isempty(cellsWithoutSegs) ?
      findmin(cellsWithSegs)[2] : rand(cellsWithoutSegs)
end

# OPTIMIZE add many segments together for efficiency?
function growseg!(synapses::DistalSynapses,maxsegs::Vector{Option{Int}},burstingcolidx)
  cellsToGrow= map(col-> leastusedcell(synapses,col), burstingcolidx[isnothing.(maxsegs)])
  columnsToGrow= cell2col(cellsToGrow,synapses.k)
  Ncell= size(synapses.Dd,1); Ncol= size(synapses.segCol,2)
  Nseg_before= size(NS(synapses),2)
  Nseggrow= length(cellsToGrow)  # grow 1 seg per cell
  _grow_synapse_matrices!(synapses,columnsToGrow,cellsToGrow,Nseggrow)
  # Replace in maxsegs, `nothing` with something :-)
  maxsegs[isnothing.(maxsegs)].= Nseg_before.+(1:Nseggrow)
end
function _grow_synapse_matrices!(synapses::DistalSynapses,columnsToGrow,cellsToGrow,Nseggrow)
  synapses.segCol= vcat!!(synapses.segCol, columnsToGrow, trues(Nseggrow))
  synapses.neurSeg= hcat!!(synapses.neurSeg, cellsToGrow,trues(Nseggrow))
  synapses.connected= hcat!!(synapses.connected, cellsToGrow,falses(Nseggrow))
  synapses.Dd= hcat!!(synapses.Dd, Nseggrow)
end

# [Dict] Count the frequency of occurence for each element in x
countmap_empty(x)= isempty(x) ? x : countmap(x)
