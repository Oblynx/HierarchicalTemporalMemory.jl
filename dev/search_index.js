var documenterSearchIndex = {"docs":
[{"location":"examples/#Examples-1","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Timeseries-prediction:-power-consumption-in-a-gym-1","page":"Examples","title":"Timeseries prediction: power consumption in a gym","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"This example reproduces a simple upstream experiment by making 1-step-ahead predictions on a single regularly-sampled time series of a building's power consumption.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"note: Note\nAlthough HTM is about modelling the brain, this example won't look into that at all and look at it simply as a predictive model.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The time series exhibits daily and weekly patterns, but also mean shifts and non-ergodicity. Here's what a small part of the time series looks like:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: power consumption timeseries)","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"We'll use this data in this example. Follow along at the source","category":"page"},{"location":"examples/#HTM-model-1","page":"Examples","title":"HTM model","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: htm model)","category":"page"},{"location":"examples/#Encoding-1","page":"Examples","title":"Encoding","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"An HTM model doesn't understand numbers for input. It needs large binary vectors that represent other neurons firing. These are called Sparse Distributed Representations (SDRs). Transforming input data into SDRs is called encoding and the important point is to maintain the semantics of the input space under this transformation: if 3.1kW is considered to be very similar to 3.2kW, then the corresponding SDRs should also be very similar. Correspondingly the SDR for 5kW should probably have no similarity.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"So what is SDR similarity? As binary vectors, \"similarity\" is how many 1s they have at the same places, or, the inner product of 2 SDRs, or bitwise(AND) then reduce(+).","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"similarity(x,y)= sum(x .& y);\nA= [0,0,1,0,1,0,1,0];\nB= [0,0,1,0,0,0,1,0];\n\nsimilarity(A,B)\n\nC= [1,1,1,0,0,1,0,1];\nsimilarity(A,C)","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"In this case the encoder is [encode_simpleArithmetic] and linearly maps a range of real numbers [a,b] to shifting bit patterns within a fixed length SDR. If the range is [0,1], it could look like:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"0   -> 1110000000\n0.2 -> 0011100000\n...\n1   -> 0000000111","category":"page"},{"location":"examples/#Spatial-Pooler-1","page":"Examples","title":"Spatial Pooler","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The SpatialPooler is a learning algorithm that decorrelates the features of an input space, producing a Sparse Distributed Representation (SDR) of the input space.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"It's called SpatialPooler because input patterns that share a large number of co-active neurons (i.e., that are spatially similar) are grouped together into a common output representation. It is designed to achieve a set of computational properties that support further downstream computations with SDRs, including:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"preserving topology of the input space by mapping similar inputs to similar outputs\ncontinuously adapting to changing statistics of the input stream\nforming fixed sparsity representations\nbeing robust to noise\nbeing fault tolerant","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Source","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"We can evaluate the \"mapping property\" of the spatial pooler, or if it manages to represent the similarities of the input space in the output:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: spatial pooler mapping property evaluation)","category":"page"},{"location":"examples/#Temporal-memory-1","page":"Examples","title":"Temporal memory","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"After the SDRs have been normalized comes the contextualizing and predictive step, the TemporalMemory. The TemporalMemory learns to predict sequences of input SDRs. It represents each input symbol in the temporal context of the symbols that come before it in the sequence and predicts its own activation in the following time step.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"This prediction is the TM's output, another SDR that we must decode to bring it back out from the model's internal representation. Decoding however is more complicated than encoding, because the only information we have to associate the TM's activations with the input symbols is that they represent the input symbol in a temporal context. What might help us is discovering a function that maps internal representations to the input symbols that they should represent.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"We can train a simple learning algorithm (eg 1-layer neural network) to decode the TM, by using the actual encodings as ground truth. If we want to predict k time steps into the future, then at time t we update the learning algorithm with error signal","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"e_t = D(Œ†_t-k) - u_t","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"where Œ† is the TM's prediction, an SDR, D(Œ†) is the decoding of the prediction, and u is the input.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Here, the decoder SDRClassifier actually outputs a probability distribution over all the encoder's classes.","category":"page"},{"location":"examples/#Prediction-results-1","page":"Examples","title":"Prediction results","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: prediction results)","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Blue: original time series\nRed: prediction 1 step ahead","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"To make the prediction above proper dimensions were chosen for the algorithms, but their various tuning parameters weren't tuned a lot. The error metric is MASE, calculated on a 10-day sliding window.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The learning parts of the system are:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Spatial Pooler\nTemporal Memory\nDecoder","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"In this example they're all learning simultaneously, but the spatial pooler's learning can be considered an optimization. The important learning components are the temporal memory and the deoder.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Before the system has learned anything the decoder outputs a more or less static value in the middle of the encoding range. Gradually, the most frequent patterns of the time series start being recognised: the daily fluctuations. Weekends take more time, as what counts is the number of times a pattern has been seen. But is the prediction good in any case?","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"After about 600 steps the HTM model does a bit better than simply predicting the current value for the next value and starts hitting the daily peaks. From step 1200 on the frequency of small unpredictable events increases and the accuracy starts dropping. However the mean shift at 1750 is handled gracefully and doesn't increase the error further. The HTM adapts to the new statistics.","category":"page"},{"location":"reference/#Package-documentation-1","page":"Reference","title":"Package documentation","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [HierarchicalTemporalMemory]","category":"page"},{"location":"reference/#HierarchicalTemporalMemory.Hypercube","page":"Reference","title":"HierarchicalTemporalMemory.Hypercube","text":"Hypercube iterates over a hypercube of radius Œ≥ around x·∂ú, bounded inside the space {1..sz}·¥∫.\n\nExamples\n\njulia> hc= Hypercube((3,3),1,(10,10));\n\njulia> foreach(println, hc)\n(2, 2)\n(3, 2)\n(4, 2)\n(2, 3)\n(3, 3)\n(4, 3)\n(2, 4)\n(3, 4)\n(4, 4)\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.Hypersphere","page":"Reference","title":"HierarchicalTemporalMemory.Hypersphere","text":"Hypersphere is approximated with a Hypercube for simplicity\n\nSee also: Hypercube\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.SPParams","page":"Reference","title":"HierarchicalTemporalMemory.SPParams","text":"SPParams{Nin,Nsp} holds the algorithm parameters for a spatial pooler with nomenclature similar to source\n\nThe dimension parameters are problem-specific and should be the first to be specified.\n\nThe tuning parameters have sensible defaults, which should be . All gated features are enabled by default.\n\nParameters\n\nDimensions\n\nsz·µ¢‚Çô, sz‚Çõ‚Çö: input/output dimensions\nŒ≥: receptive field radius (how large an input area an output minicolumn maps to)\n\nAlgorithm tuning\n\ns: average output sparsity\nprob_synapse: probability for each element of the sz·µ¢‚Çô √ó sz‚Çõ‚Çö space to be a synapse. Elements that roll below this value don't form a synapse and don't get a permanence value. If this is very low, the proximal synapses matrix can become sparse.\nŒ∏_permanence01: synapse permanence connection threshold\np‚Å∫_01,p‚Åª_01: synapse permanence adaptation rate (see ProximalSynapses)\nŒ∏_stimulus_activate: minicolumn absolute activation threshold\nTboost: boosting mechanism's moving average filter period\nŒ≤: boosting strength\n\nFeature gates\n\nenable_local_inhibit\nenable_learning\nenable_boosting\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.SpatialPooler","page":"Reference","title":"HierarchicalTemporalMemory.SpatialPooler","text":"SpatialPooler{Nin,Nsp} is a learning algorithm that decorrelates the features of an input space, producing a Sparse Distributed Representation (SDR) of the input space. If defines the proximal connections of an HTM layer.\n\nExamples\n\nsp= SpatialPooler(SPParams(sz·µ¢‚Çô=(600,), sz‚Çõ‚Çö=(2048,)))\nz= Random.bitrand(600)  # input\nactivation= sp(z)\n# or, to adapt the SP to the input as well:\nactivation= step!(sp,z)\n\nProperties\n\nIt's called SpatialPooler because input patterns that share a large number of co-active neurons (i.e., that are spatially similar) are grouped together into a common output representation. It is designed to achieve a set of computational properties that support further downstream computations with SDRs, including:\n\npreserving topology of the input space by mapping similar inputs to similar outputs\ncontinuously adapting to changing statistics of the input stream\nforming fixed sparsity representations\nbeing robust to noise\nbeing fault tolerant\n\nSource\n\nAlgorithm overview\n\nMapping I/O spaces\n\nThe spatial pooler maps an input space x to an output space y of minicolumns through a matrix of proximal synapses. The input space can optionally have a topology, which the spatial pooler will preserve by mapping output minicolumn y·µ¢ to a subset of the input space, a Hypercube around center x·∂ú. julia x·∂ú(y·µ¢)= floor.(Int, (y·µ¢.-1) .* (sz·µ¢‚Çô./sz‚Çõ‚Çö)) .+1`\n\nOutput activation\n\nCalculate the overlap o(y·µ¢) by propagating the input activations through the proximal synapses and adjusting by boosting factors b (control mechanism that spreads out the activation pattern across understimulated neurons)\nInhibition Z between y·µ¢ (local/global): competition where only the top-K y·µ¢ with the highest o(y·µ¢) win; ensures sparsity\nActivate winning y·µ¢ > activation threshold (Œ∏_stimulus_activate)\n\nSee also: sp_activate\n\nState variables\n\nsynapses: includes the synapse permanence matrix D‚Çö\n√•‚Çú: [boosting] moving (in time) average (in time) activation of each minicolumn\n√•‚Çô: [boosting] moving (in time) average (in neighborhood) activation of each minicolumn\nœÜ: the adaptible radius of local inhibition\n\n\n\nSee also: ProximalSynapses, TemporalMemory\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.SpatialPooler-Tuple{Any}","page":"Reference","title":"HierarchicalTemporalMemory.SpatialPooler","text":"Return the activation pattern of the SpatialPooler for the given input activation.\n\nz: CellActivity\n\nExample\n\nsp= SpatialPooler(SPParams(sz·µ¢‚Çô=(600,), sz‚Çõ‚Çö=(2048,)))\nz= Random.bitrand(600)  # input\nactivation= sp(z)\n# or, to adapt the SP to the input as well:\nactivation= step!(sp,z)\n\nFor details see: sp_activate\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.TMParams","page":"Reference","title":"HierarchicalTemporalMemory.TMParams","text":"TMParams holds the algorithm parameters for a Temporal Memory with nomenclature similar to source\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.TemporalMemory","page":"Reference","title":"HierarchicalTemporalMemory.TemporalMemory","text":"TemporalMemory learns to predict sequences of input Sparse Distributed Representations (SDRs), usually generated by a SpatialPooler. It learns to represent each input symbol in the temporal context of the symbols that come before it in the sequence, using the individual neurons of each minicolumn and their distal synapses.\n\nWhen considering a neuron layer with proximal and distal synapses, the spatial pooler is a way to activate and learn the proximal synapses, while the temporal memory is a way to activate and learn the distal synapses.\n\nTM activation\n\nAn SDR input activates some minicolumns of the neuron layer.\n\n\n\nSee also: TMState\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.predict!-Tuple{SDRClassifier,Any,Int64}","page":"Reference","title":"HierarchicalTemporalMemory.predict!","text":"predict!(classifier::SDRClassifier, Œ†,target::Int; enable_learning=true)=\n\nPredict probability that the output represents each bucket of an arithmetic encoder. 0<=predict!<=1\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.reverse_simpleArithmetic-Tuple{Array{T,1} where T,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.reverse_simpleArithmetic","text":"reverse_simpleArithmetic(bucketProbDist, algorithm,params)\n\nReverses the simple arithmetic encoder, given the same parameters. Inputs a probability distribution across all buckets and collapses it to a single arithmetic value, representing the most likely estimation in the timeseries' domain (like a defuzzifier)\n\nArguments\n\nbucketProbDist [nbucket]: discrete probability of each bucket [0-1]\nalgorithm: {'mean','mode','highmean'} 'highmean' is the mean of the highest-estimated values\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.step!-Tuple{HierarchicalTemporalMemory.ProximalSynapses,Any,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.step!","text":"step!(s::ProximalSynapses, z,a, params) adapts the proximal synapses' permanences with a hebbian learning rule on input z and activation a. The adaptation has a causal and an anticausal component:\n\nIf the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened\nIf the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened\n\nSee alse: ProximalSynapses\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.step!-Tuple{SpatialPooler,Any}","page":"Reference","title":"HierarchicalTemporalMemory.step!","text":"step!(sp::SpatialPooler, z::CellActivity) evolves the Spatial Pooler to the next timestep by evolving each of its constituents (synapses, boosting, inhibition radius) and returns the output activation.\n\nSee also: sp_activate\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.ùïäùï¢range","page":"Reference","title":"HierarchicalTemporalMemory.ùïäùï¢range","text":"ùïäùï¢range is the domain of connection permanence quantization\n\n\n\n\n\n","category":"constant"},{"location":"reference/#HierarchicalTemporalMemory.CellActivity","page":"Reference","title":"HierarchicalTemporalMemory.CellActivity","text":"Type of neuron layer activations\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.InhibitionRadius","page":"Reference","title":"HierarchicalTemporalMemory.InhibitionRadius","text":"The inhibition radius of a Spatial Pooler's columns is a dynamical system that evolves under the influence of other elements of the Spatial Pooler. It provides an init (constructor) and a step! function.\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.LengthfulIter","page":"Reference","title":"HierarchicalTemporalMemory.LengthfulIter","text":"Iterator transformations, like filters, don't keep the length info. LengthfulIter is a wrapper that allows length info known programmatically to be used.\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.ProximalSynapses","page":"Reference","title":"HierarchicalTemporalMemory.ProximalSynapses","text":"ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection} are the feedforward connections of 2 neuron layers.\n\nUsed in the context of the SpatialPooler.\n\nDescription\n\nThe neurons of both layers are expected to form minicolumns which share the same feedforward connections. The synapses are binary: they don't have a scalar weight, but either conduct (1) or not (0). Instead, they have a permanence value D‚Çö ‚àà (0,1] and a connection threshold Œ∏.\n\nInitialization\n\nLet presynaptic (input) neuron x·µ¢ and postsynaptic (output) neuron y·µ¢, and a topological I/O mapping x·µ¢(y·µ¢) := Hypercube(y·µ¢). ‚àÄ\n\nSynapse adaptation\n\nThey adapt with a hebbian learning rule. The adaptation has a causal and an anticausal component:\n\nIf the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened\nIf the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened\n\nThe synaptic permanences are clipped at the boundaries of 0 and 1.\n\nA simple implementation of the learning rule would look like this, where z: input, a: output\n\nlearn!(D‚Çö,z,a)= begin\n  D‚Çö[z,a]  .= (D‚Çö[z,a].>0) .* (D‚Çö[z,a]   .‚äï p‚Å∫)\n  D‚Çö[.!z,a].= (D‚Çö[z,a].>0) .* (D‚Çö[.!z,a] .‚äñ p‚Åª)\nend\n\nType parameters\n\nThey allow a dense or sparse matrix representation of the synapses\n\nSynapseT: DenseSynapses or SparseSynapses\nConnectedT: DenseConnection or SparseConnection\n\nSee also: DistalSynapses, SpatialPooler, TemporalMemory\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.Sequence","page":"Reference","title":"HierarchicalTemporalMemory.Sequence","text":"Sequence(;Œ¥,init) is an easy way to define sequences with transition function Œ¥ and starting condition init as an iterator.\n\nExamples\n\nFibonacci sequence:\n\nfibonacci_Œ¥(a,b)= b, a+b;\nfibonacci_init= 0,1;\nLazy.@as _1 HTM.Sequence(fibonacci_Œ¥, fibonacci_init) IterTools.take(_1, 13) foreach(println, _1)\n\n# output\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\n55\n89\n144\n\nFactorial sequence:\n\nfactorial_Œ¥(a,b)= a*b, b+1;\nfactorial_init= 1,1;\nLazy.@as _1 HTM.Sequence(factorial_Œ¥, factorial_init) IterTools.take(_1, 13) foreach(println, _1)\n\n# output\n1\n1\n2\n6\n24\n120\n720\n5040\n40320\n362880\n3628800\n39916800\n479001600\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.Truesof","page":"Reference","title":"HierarchicalTemporalMemory.Truesof","text":"Iterate over the trues of a BitArray\n\nExamples\n\njulia> b= Random.bitrand(5)\n5-element BitArray{1}:\n 1\n 0\n 0\n 1\n 1\n\njulia> foreach(i-> print(string(i)*\" \"), HTM.Truesof(b))\n1 4 5\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.ùïäùï¢","page":"Reference","title":"HierarchicalTemporalMemory.ùïäùï¢","text":"ùïäùï¢ is the type of connection permanences, defining their quantization domain\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.bitarray-Tuple{Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.bitarray","text":"bitarray(dims, idx)\n\nCreate a bitarray with true only at idx.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.hcat!!-Tuple{SparseArrays.SparseMatrixCSC,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.hcat!!","text":"hcat!!(s::SparseMatrixCSC, I,V)\n\nWARNING: s is invalid after this operation!!! Its symbol should be reassigned to the output of hcat!!. Append new columns to s with 1 value V[c] at row I[c] each. Because SparseMatrixCSC is immutable, return a new, valid SparseMatrixCSC that points to the original's data structures. For large matrices, this is much faster than hcat.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.sp_activate-Union{Tuple{Nsp}, Tuple{Nin}, Tuple{SpatialPooler{Nin,Nsp},Any}} where Nsp where Nin","page":"Reference","title":"HierarchicalTemporalMemory.sp_activate","text":"sp_activate(sp::SpatialPooler{Nin,Nsp}, z) calculates the SP's output activation for given input activation z.\n\nAlgorithm\n\nOverlap o(y·µ¢)\nPropagate the input activations through the proximal synapses (matrix multiply)\nApply boosting factors b: control mechanism that spreads out the activation pattern across understimulated neurons (homeostatic excitability control)\nInhibition Z between y·µ¢ (local/global): competition where only the top-k y·µ¢ with the highest o(y·µ¢) win; ensures sparsity The competition happens within an area around each neuron.\nk: number of winners depends on desired sparsity (s, see SPParams) and area size\nŒ∏_inhibit: inhibition threshold per neighborhood = o(k-th y)\nZ(o(y·µ¢)): convolution of o with Œ∏_inhibit\nActivate winning y·µ¢ > activation threshold (Œ∏_stimulus_activate)\n\ninfo: Info\nThe local sparsity s tends to the sparsity of the entire layer as it grows larger, but for small values or small inhibition radius it diverges, because of the limited & integral number of neurons winning in each neighborhood. This could be addressed by tie breaking, but it doesn't seem to have much practical importance.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.sparse_foreach-Tuple{Any,SparseArrays.SparseMatrixCSC,Any}","page":"Reference","title":"HierarchicalTemporalMemory.sparse_foreach","text":"sparse_foreach(f, s::SparseMatrixCSC,columnIdx)\n\nIterate SparseMatrix s and apply f columnwise (f(s,nzrange,rowvals))\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.step_boost!-Tuple{SpatialPooler,Any}","page":"Reference","title":"HierarchicalTemporalMemory.step_boost!","text":"step_boost!(sp::SpatialPooler,a) evolves the boosting factors b (see sp_activate). They depend on:\n\n√•‚Çú: moving average in time activation of each minicolumn\n√•‚Çô: moving average in neighborhood activation of each minicolum\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.vcat!!-Tuple{SparseArrays.SparseMatrixCSC,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.vcat!!","text":"vcat!!(s::SparseMatrixCSC, J,V)\n\nWARNING: s is invalid after this operation!!! Its symbol should be reassigned to the output of vcat!!. Append new rows to s with 1 value V[r] at column J[r] each. Because SparseMatrixCSC is immutable, return a new, valid SparseMatrixCSC that points to the original's data structures. For large matrices, this is much faster than vcat.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.@percolumn-NTuple{4,Any}","page":"Reference","title":"HierarchicalTemporalMemory.@percolumn","text":"@percolumn(f,a,b,k,Ncol)\n\nMacro to apply f elementwise and concatenate the results.\n\na: vector of size [Ncol*k], column-major\nb: vectors of size Ncol\n\n\n\n\n\n","category":"macro"},{"location":"reference/#HierarchicalTemporalMemory.@percolumn-Tuple{Any,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.@percolumn","text":"@percolumn(reduce,a,k,Ncol)\n\nMacro to reduce a per column.\n\n\n\n\n\n","category":"macro"},{"location":"#","page":"Home","title":"Home","text":"CurrentModule = HierarchicalTemporalMemory","category":"page"},{"location":"#Hierarchical-Temporal-Memory-1","page":"Home","title":"Hierarchical Temporal Memory","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Hierarchical Temporal Memory is an abstract algorithmic model of the human brain (specifically the neocortex). It's a tool for","category":"page"},{"location":"#","page":"Home","title":"Home","text":"neuroscience: understanding the human brain\nmachine learning: predicting time series and detecting anomalies","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The main algorithms of this model, the Spatial Pooler and Temporal (Sequence) Memory, are described in:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The HTM Spatial Pooler‚ÄîA Neocortical Algorithm for Online Sparse Distributed Coding\nContinuous Online Sequence Learning with an Unsupervised Neural Network Model (section 3.3)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The official implementation of this model in Python (NUPIC) serves as a reference point and as source of truth for many implementation details, but it doesn't take advantage of the data-driven design that the source material encourages and ends up quite verbose. This implementation uses Julia's expressivity to remain faithful to the papers' terminology, attempting to express the algorithms more simply and concisely, and thus instigate further research on them. Some essential features of this expressivity are broadcasting, duck-typed interfaces and unicode source code.","category":"page"},{"location":"#What-is-the-HTM?-1","page":"Home","title":"What is the HTM?","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The Hierarchical Temporal Memory (HTM) is a biologically constrained theory, aiming primarily to model the function of the neocortex (a structure of the human brain), and as a secondary goal, machine learning applications. The algorithms expose many fundamental properties which are used to test this implementation.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"In contrast to many established neural network models, the HTM neuron performs coincidence detection across multiple input \"dendrites\". HTM is actually closer to a spiking neural network, with binary synapses and signals, that encodes information in population codes. Spatial pooling and the temporal memory are both unsupervised processes that adapt continuously; together, they learn to identify sequences in noisy input time series.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"HTM theory is not yet complete, lacking a definitive way to stabilize sequence representations and compose small models. Exactly for this reason, we believe that a concise and high level model can accelerate the research.","category":"page"},{"location":"#Roadmap-1","page":"Home","title":"Roadmap","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Timeseries prediction tests, NAB results\nSensorimotor inference and temporal pooling ideas, influenced by forum discussions such as this. This goes in the direction of the biggest current problems:\ntemporal noise\nhierarchical model composition","category":"page"}]
}
