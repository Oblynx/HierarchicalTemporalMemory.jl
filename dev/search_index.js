var documenterSearchIndex = {"docs":
[{"location":"examples/#Examples-1","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Timeseries-prediction:-power-consumption-in-a-gym-1","page":"Examples","title":"Timeseries prediction: power consumption in a gym","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"This example reproduces a simple upstream experiment by making 1-step-ahead predictions on a single regularly-sampled time series of a building's power consumption.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"note: Note\nAlthough HTM is about modelling the brain, this example won't look into that at all and look at it simply as a predictive model.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The time series exhibits daily and weekly patterns, but also mean shifts and non-ergodicity. Here's what a small part of the time series looks like:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: power consumption timeseries)","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"We'll use this data in this example. Follow along at the source","category":"page"},{"location":"examples/#HTM-model-1","page":"Examples","title":"HTM model","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: htm model)","category":"page"},{"location":"examples/#Encoding-1","page":"Examples","title":"Encoding","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"An HTM model doesn't understand numbers for input. It needs large binary vectors that represent other neurons firing. These are called Sparse Distributed Representations (SDRs). Transforming input data into SDRs is called encoding and the important point is to maintain the semantics of the input space under this transformation: if 3.1kW is considered to be very similar to 3.2kW, then the corresponding SDRs should also be very similar. Correspondingly the SDR for 5kW should probably have no similarity.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"So what is SDR similarity? As binary vectors, \"similarity\" is how many 1s they have at the same places, or, the inner product of 2 SDRs, or bitwise(AND) then reduce(+).","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"similarity(x,y)= sum(x .& y);\nA= [0,0,1,0,1,0,1,0];\nB= [0,0,1,0,0,0,1,0];\n\nsimilarity(A,B)\n\nC= [1,1,1,0,0,1,0,1];\nsimilarity(A,C)","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"In this case the encoder is [encode_simpleArithmetic] and linearly maps a range of real numbers [a,b] to shifting bit patterns within a fixed length SDR. If the range is [0,1], it could look like:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"0   -> 1110000000\n0.2 -> 0011100000\n...\n1   -> 0000000111","category":"page"},{"location":"examples/#Spatial-Pooler-1","page":"Examples","title":"Spatial Pooler","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The SpatialPooler is a learning algorithm that decorrelates the features of an input space, producing a Sparse Distributed Representation (SDR) of the input space.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"It's called SpatialPooler because input patterns that share a large number of co-active neurons (i.e., that are spatially similar) are grouped together into a common output representation. It is designed to achieve a set of computational properties that support further downstream computations with SDRs, including:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"preserving topology of the input space by mapping similar inputs to similar outputs\ncontinuously adapting to changing statistics of the input stream\nforming fixed sparsity representations\nbeing robust to noise\nbeing fault tolerant","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Source","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"We can evaluate the \"mapping property\" of the spatial pooler, or if it manages to represent the similarities of the input space in the output:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: spatial pooler mapping property evaluation)","category":"page"},{"location":"examples/#Temporal-memory-1","page":"Examples","title":"Temporal memory","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"After the SDRs have been normalized comes the contextualizing and predictive step, the TemporalMemory. The TemporalMemory learns to predict sequences of input SDRs. It represents each input symbol in the temporal context of the symbols that come before it in the sequence and predicts its own activation in the following time step.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"This prediction is the TM's output, another SDR that we must decode to bring it back out from the model's internal representation. Decoding however is more complicated than encoding, because the only information we have to associate the TM's activations with the input symbols is that they represent the input symbol in a temporal context. What might help us is discovering a function that maps internal representations to the input symbols that they should represent.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"We can train a simple learning algorithm (eg 1-layer neural network) to decode the TM, by using the actual encodings as ground truth. If we want to predict k time steps into the future, then at time t we update the learning algorithm with error signal","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"e_t = D(Π_t-k) - u_t","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"where Π is the TM's prediction, an SDR, D(Π) is the decoding of the prediction, and u is the input.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Here, the decoder SDRClassifier actually outputs a probability distribution over all the encoder's classes.","category":"page"},{"location":"examples/#Prediction-results-1","page":"Examples","title":"Prediction results","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"(Image: prediction results)","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Blue: original time series\nRed: prediction 1 step ahead","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"To make the prediction above proper dimensions were chosen for the algorithms, but their various tuning parameters weren't tuned a lot. The error metric is MASE, calculated on a 10-day sliding window.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The learning parts of the system are:","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Spatial Pooler\nTemporal Memory\nDecoder","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"In this example they're all learning simultaneously, but the spatial pooler's learning can be considered an optimization. The important learning components are the temporal memory and the deoder.","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Before the system has learned anything the decoder outputs a more or less static value in the middle of the encoding range. Gradually, the most frequent patterns of the time series start being recognised: the daily fluctuations. Weekends take more time, as what counts is the number of times a pattern has been seen. But is the prediction good in any case?","category":"page"},{"location":"examples/#","page":"Examples","title":"Examples","text":"After about 600 steps the HTM model does a bit better than simply predicting the current value for the next value and starts hitting the daily peaks. From step 1200 on the frequency of small unpredictable events increases and the accuracy starts dropping. However the mean shift at 1750 is handled gracefully and doesn't increase the error further. The HTM adapts to the new statistics.","category":"page"},{"location":"reference/#Package-documentation-1","page":"Reference","title":"Package documentation","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [HierarchicalTemporalMemory]","category":"page"},{"location":"reference/#HierarchicalTemporalMemory.Hypercube","page":"Reference","title":"HierarchicalTemporalMemory.Hypercube","text":"Hypercube iterates over a hypercube of radius γ around xᶜ, bounded inside the space {1..sz}ᴺ.\n\nExamples\n\njulia> hc= Hypercube((3,3),1,(10,10));\n\njulia> foreach(println, hc)\n(2, 2)\n(3, 2)\n(4, 2)\n(2, 3)\n(3, 3)\n(4, 3)\n(2, 4)\n(3, 4)\n(4, 4)\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.Hypersphere","page":"Reference","title":"HierarchicalTemporalMemory.Hypersphere","text":"Hypersphere is approximated with a Hypercube for simplicity\n\nSee also: Hypercube\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.SPParams","page":"Reference","title":"HierarchicalTemporalMemory.SPParams","text":"SPParams holds the algorithm parameters for a spatial pooler with nomenclature similar to source\n\nThe dimension parameters are problem-specific and should be the first to be specified.\n\nThe tuning parameters have sensible defaults, which should be . All gated features are enabled by default.\n\nParameters\n\nDimensions\n\nszᵢₙ = (32,32): input dimensions\nszₛₚ = (32,32): output dimensions\nγ = 6: receptive field radius (how large an input area an output minicolumn maps to). Must be <= min(szᵢₙ)\n\nAlgorithm tuning\n\ns = .02: average output sparsity\nprob_synapse = .5: probability for each element of the szᵢₙ × szₛₚ space to be a synapse. Elements that roll below this value don't form a synapse and don't get a permanence value. If this is very low, the proximal synapses matrix can become sparse.\nθ_permanence01 = .5: synapse permanence connection threshold\np⁺_01 = .1 , p⁻_01 = .02: synapse permanence adaptation rate (see ProximalSynapses)\nθ_stimulus_activate = 1: minicolumn absolute activation threshold\nTboost = 200.0: boosting mechanism's moving average filter period\nβ = 1.0: boosting strength\n\nFeature gates\n\nenable_local_inhibit = true\nenable_learning = true\nenable_boosting = true\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.SpatialPooler","page":"Reference","title":"HierarchicalTemporalMemory.SpatialPooler","text":"SpatialPooler is a learning algorithm that decorrelates the features of an input space, producing a Sparse Distributed Representation (SDR) of the input space. If defines the proximal connections of an HTM layer.\n\nExamples\n\nsp= SpatialPooler(SPParams(szᵢₙ=(600,), szₛₚ=(2048,)))\nz= Random.bitrand(600)  # input\nactivation= sp(z)\n# or, to adapt the SP to the input as well:\nactivation= step!(sp,z)\n\nProperties\n\nIt's called SpatialPooler because input patterns that share a large number of co-active neurons (i.e., that are spatially similar) are grouped together into a common output representation. It is designed to achieve a set of computational properties that support further downstream computations with SDRs, including:\n\npreserving topology of the input space by mapping similar inputs to similar outputs\ncontinuously adapting to changing statistics of the input stream\nforming fixed sparsity representations\nbeing robust to noise\nbeing fault tolerant\n\nSource\n\nAlgorithm overview\n\nMapping I/O spaces\n\nThe spatial pooler maps an input space x to an output space y of minicolumns through a matrix of proximal synapses. The input space can optionally have a topology, which the spatial pooler will preserve by mapping output minicolumn yᵢ to a subset of the input space, a Hypercube around center xᶜ. julia xᶜ(yᵢ)= floor.(Int, (yᵢ.-1) .* (szᵢₙ./szₛₚ)) .+1`\n\nOutput activation\n\nCalculate the overlap o(yᵢ) by propagating the input activations through the proximal synapses and adjusting by boosting factors b (control mechanism that spreads out the activation pattern across understimulated neurons)\nInhibition Z between yᵢ (local/global): competition where only the top-K yᵢ with the highest o(yᵢ) win; ensures sparsity\nActivate winning yᵢ > activation threshold (θ_stimulus_activate)\n\nSee also: sp_activate\n\nState variables\n\nsynapses: includes the synapse permanence matrix Dₚ\nåₜ: [boosting] moving (in time) average (in time) activation of each minicolumn\nåₙ: [boosting] moving (in time) average (in neighborhood) activation of each minicolumn\nφ: the adaptible radius of local inhibition\n\n\n\nSee also: ProximalSynapses, TemporalMemory\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.SpatialPooler-Tuple{Any}","page":"Reference","title":"HierarchicalTemporalMemory.SpatialPooler","text":"Return the activation pattern of the SpatialPooler for the given input activation.\n\nz: CellActivity\n\nExample\n\nsp= SpatialPooler(SPParams(szᵢₙ=(600,), szₛₚ=(2048,)))\nz= Random.bitrand(600)  # input\nactivation= sp(z)\n# or, to adapt the SP to the input as well:\nactivation= step!(sp,z)\n\nFor details see: sp_activate\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.TMParams","page":"Reference","title":"HierarchicalTemporalMemory.TMParams","text":"TMParams holds the algorithm parameters for a Temporal Memory with nomenclature similar to source\n\nDimensions\n\nNc = 2500: number of columns\nk = 10: cells per column\nNₙ = k Nc neurons in layer. The Nₛ number of dendritic segments is variable\n\nTuning\n\np⁺_01 = .12 , p⁻_01 = .04 ∈ [0,1]: synapse permanence adaptation rate (see ProximalSynapses)\nLTD_p⁻_01 = .002 ∈ [0,1]: synapse long term depression rate\nθ_permanence = .5*typemax(𝕊𝕢) ∈ 𝕊𝕢: synapse permanence connection threshold\ninit_permanence = .4*typemax(𝕊𝕢) ∈ 𝕊𝕢: permanence of a newly-grown synapse\nsynapseSampleSize = 25 ∈ ℕ: target number of matching synapses per dendrite. Represents how many bits the dendrite targets to recognize the input. Dendrites with fewer synapses matching the input might grow new synapses.\nθ_stimulus_activate = 14 ∈ ℕ: number of matching synapses needed to depolarize the dendrite\nθ_stimulus_learn = 12 ∈ ℕ: number of matching synapses that are insufficient to depolarize the dendrite, but sufficient to trigger learning. θ_stimulus_learn <= θ_stimulus_activate\n\nFeature gates\n\nenable_learning = true\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.TemporalMemory","page":"Reference","title":"HierarchicalTemporalMemory.TemporalMemory","text":"TemporalMemory learns to predict sequences of input Sparse Distributed Representations (SDRs), usually generated by a SpatialPooler. It learns to represent each input symbol in the temporal context of the symbols that come before it in the sequence, using the individual neurons of each minicolumn and their distal synapses.\n\nWhen considering a neuron layer with proximal and distal synapses, the spatial pooler is a way to activate and learn the proximal synapses, while the temporal memory is a way to activate and learn the distal synapses.\n\nHigh-order predictions and Ambiguity\n\nThe neurons-thousand-synapses paper describes the Temporal Memory's properties, and especially the ability to\n\nmake \"high-order\" predictions, based on previous inputs potentially going far back in time\nrepresent ambiguity in the form of simultaneous predictions\n\nFor more information see figures 2,3 in the paper.\n\nTM activation\n\nOverview of the temporal memory's process:\n\nActivate neurons (fire, proximal synapses)\nPredict neurons (depolarize, distal/apical synapses)\nLearn distal/apical synapses:\nadapt existing synapses\ncreate new synapses/dendrites\n\nActivation\n\nAn SDR input activates some minicolumns of the neuron layer. If some neurons in the minicolum were predicted at the previous step, they activate faster than the rest and inhibit them. If no neuron was predicted, all the neurons fire (minicolumn bursting).\n\nTODO\n\n\n\nSee also: TMParams for parameter and symbol description, DistalSynapses, TMState\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.predict!-Tuple{SDRClassifier,Any,Int64}","page":"Reference","title":"HierarchicalTemporalMemory.predict!","text":"predict!(classifier::SDRClassifier, Π,target::Int; enable_learning=true)=\n\nPredict probability that the output represents each bucket of an arithmetic encoder. 0<=predict!<=1\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.reverse_simpleArithmetic-Tuple{Array{T,1} where T,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.reverse_simpleArithmetic","text":"reverse_simpleArithmetic(bucketProbDist, algorithm,params)\n\nReverses the simple arithmetic encoder, given the same parameters. Inputs a probability distribution across all buckets and collapses it to a single arithmetic value, representing the most likely estimation in the timeseries' domain (like a defuzzifier)\n\nArguments\n\nbucketProbDist [nbucket]: discrete probability of each bucket [0-1]\nalgorithm: {'mean','mode','highmean'} 'highmean' is the mean of the highest-estimated values\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.step!-Tuple{HierarchicalTemporalMemory.ProximalSynapses,Any,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.step!","text":"step!(s::ProximalSynapses, z,a, params) adapts the proximal synapses' permanences with a hebbian learning rule on input z and activation a. The adaptation has a causal and an anticausal component:\n\nIf the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened\nIf the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened\n\nSee alse: ProximalSynapses\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.step!-Tuple{SpatialPooler,Any}","page":"Reference","title":"HierarchicalTemporalMemory.step!","text":"step!(sp::SpatialPooler, z::CellActivity) evolves the Spatial Pooler to the next timestep by evolving each of its constituents (synapses, boosting, inhibition radius) and returns the output activation.\n\nSee also: sp_activate\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.𝕊𝕢range","page":"Reference","title":"HierarchicalTemporalMemory.𝕊𝕢range","text":"𝕊𝕢range is the domain of connection permanence quantization\n\n\n\n\n\n","category":"constant"},{"location":"reference/#HierarchicalTemporalMemory.CellActivity","page":"Reference","title":"HierarchicalTemporalMemory.CellActivity","text":"Type of neuron layer activations\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.DistalSynapses","page":"Reference","title":"HierarchicalTemporalMemory.DistalSynapses","text":"DistalSynapses are lateral connections within a neuron layer that attach to the dendrites of neurons, not directly to the soma (neuron's center), and can therefore depolarize neurons but can't activate them. Compare with ProximalSynapses.\n\nUsed in the context of the TemporalMemory.\n\nDescription\n\nSynapses\n\nNeurons have multiple signal integration zones: the soma, proximal dendrites, apical dendrites. Signals are routed to the proximal dendrites through distal synapses. This type defines both the synapses themselves and the neuron's dendrites. The synapses themselves, like the ProximalSynapses, are binary connections without weights. They have a permanence value, and above a threshold they are connected.\n\nThe synapses can be represented as an adjacency matrix of dimensions Nₙ × Nₛ: presynaptic neurons -> postsynaptic dendritic segments. This matrix is derived from the synapse permanence matrix D_d  mathit𝕊𝕢^Nₙ  Nₛ, which is sparse (eg 0.5% synapses). This affects the implementation of all low-level operations.\n\nDendritic segments\n\nNeurons have multiple dendritic segments carrying the distal synapses, each sufficient to depolarize the neuron (make it predictive). The neuron/segment adjacency matrix neurSeg (aka NS) also has dimensions Nₙ × Nₛ.\n\nLearning\n\nInstead of being randomly initialized like the proximal synapses, distal synapses and dendrite segments are grown on demand: when minicolumns can't predict their activation and burst, they trigger a growth process.\n\nThe synaptic permanence itself adapts similar to the proximal synapses: synapses that correctly predict are increased, synapses that incorrectly predict are decreased. However it's a bit more complicated to define the successful distal syapses than the proximal synapses. \"Winning segments\" WS will adapt their synapses towards \"winning neurons\" WN. Since synapses are considered directional, neurons are always presynaptic and segments postsynaptic.\n\nWinning segments are those that were predicted and then activated. Also, for every bursting minicolumn the dendrite that best \"matches\" the input will become winner. If there is no sufficiently matching dendrite, a new onw will grow on the neuron that has the fewest.\n\nWinning neurons are again those that were predicted and then activated. Among the bursting minicolumns, the neurons bearing the winning segments are the winners. Both definitions of winners are aligned with establishing a causal relationship: prediction -> activation.\n\nNew synapses grow from WS towards a random sample of WN at every step. Strongly matching segments have a lower chance to grow new synapses than weakly matching segments.\n\nnote: Note\nAdding new synapses at random indices of Dd::SparseMatrixCSC is a performance bottleneck, because it involves moving the matrix's existing data. An implementation of SparseMatrixCSC with hashmaps could help, such as SimpleSparseArrays.jl\n\nCaching\n\nThe state of the DistalSynapses is determined by the 2 matrices D_d mathitNS. A few extra matrices are filled in over the evolution of the distal synapses to accelerate the computations:\n\nconnected caches Dd > θ_permanence\nsegCol caches the segment - column map (aka SC)\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.LengthfulIter","page":"Reference","title":"HierarchicalTemporalMemory.LengthfulIter","text":"LengthfulIter is an iterator wrapper that allows length info known programmatically to be used. It's used because iterator transformations, like filters, don't keep the length info, which might however be known to the programmer.\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.ProximalSynapses","page":"Reference","title":"HierarchicalTemporalMemory.ProximalSynapses","text":"ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection} are the feedforward connections between 2 neuron layers, which can activate neurons and cause them to fire.\n\nUsed in the context of the SpatialPooler.\n\nDescription\n\nThe neurons of both layers are expected to form minicolumns which share the same feedforward connections. The synapses are binary: they don't have a scalar weight, but either conduct (1) or not (0). Instead, they have a permanence value Dₚ ∈ (0,1] and a connection threshold θ.\n\nInitialization\n\nLet presynaptic (input) neuron xᵢ and postsynaptic (output) neuron yᵢ, and a topological I/O mapping xᵢ(yᵢ) := Hypercube(yᵢ). ∀\n\nSynapse adaptation\n\nThey adapt with a hebbian learning rule. The adaptation has a causal and an anticausal component:\n\nIf the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened\nIf the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened\n\nThe synaptic permanences are clipped at the boundaries of 0 and 1.\n\nA simple implementation of the learning rule would look like this, where z: input, a: output\n\nlearn!(Dₚ,z,a)= begin\n  Dₚ[z,a]  .= (Dₚ[z,a].>0) .* (Dₚ[z,a]   .⊕ p⁺)\n  Dₚ[.!z,a].= (Dₚ[z,a].>0) .* (Dₚ[.!z,a] .⊖ p⁻)\nend\n\nType parameters\n\nThey allow a dense or sparse matrix representation of the synapses\n\nSynapseT: DenseSynapses or SparseSynapses\nConnectedT: DenseConnection or SparseConnection\n\nSee also: DistalSynapses, SpatialPooler, TemporalMemory\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.Sequence","page":"Reference","title":"HierarchicalTemporalMemory.Sequence","text":"Sequence(;δ,init) is an easy way to define sequences with transition function δ and starting condition init as an iterator.\n\nExamples\n\nFibonacci sequence:\n\nfibonacci_δ(a,b)= b, a+b;\nfibonacci_init= 0,1;\nLazy.@as _1 HTM.Sequence(fibonacci_δ, fibonacci_init) IterTools.take(_1, 13) foreach(println, _1)\n\n# output\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\n55\n89\n144\n\nFactorial sequence:\n\nfactorial_δ(a,b)= a*b, b+1;\nfactorial_init= 1,1;\nLazy.@as _1 HTM.Sequence(factorial_δ, factorial_init) IterTools.take(_1, 13) foreach(println, _1)\n\n# output\n1\n1\n2\n6\n24\n120\n720\n5040\n40320\n362880\n3628800\n39916800\n479001600\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.TMState","page":"Reference","title":"HierarchicalTemporalMemory.TMState","text":"TMState is a named tuple of the state variables of a temporal memory.\n\nα: active neurons\nΠ: predictive neurons\nWN: winning neurons\nΠₛ: predictive dendritic segments (to calculate winning segments)\nMₛ: matching dendritic segments (didn't receive enough input to activate, but enough to learn)\novp_Mₛ:\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.Truesof","page":"Reference","title":"HierarchicalTemporalMemory.Truesof","text":"TruesOf iterates over the trues of a BitArray\n\nExamples\n\njulia> b= Random.bitrand(5)\n5-element BitArray{1}:\n 1\n 0\n 0\n 1\n 1\n\njulia> foreach(i-> print(string(i)*\" \"), HTM.Truesof(b))\n1 4 5\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.𝕊𝕢","page":"Reference","title":"HierarchicalTemporalMemory.𝕊𝕢","text":"𝕊𝕢 is the type of connection permanences, defining their quantization domain\n\n\n\n\n\n","category":"type"},{"location":"reference/#HierarchicalTemporalMemory.adapt_sparsesynapses!-NTuple{5,Any}","page":"Reference","title":"HierarchicalTemporalMemory.adapt_sparsesynapses!","text":"adapt_sparsesynapses!(synapses_activeCol,input_i,z,p⁺,p⁻) updates the permanence of the given vector of synapses, which is typically a @view into the nonzero elements that represent an active column of the sparse array of synapses. The input\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.bestmatch-Tuple{HierarchicalTemporalMemory.DistalSynapses,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.bestmatch","text":"bestmatch(s::DistalSynapses,col, povp_Mₛ) finds the best-matching segment in a column, as long as its overlap with the input (its activation) is > θstimuluslearn. Otherwise, returns nothing.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.bitarray-Tuple{Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.bitarray","text":"bitarray(dims, idx)\n\nCreate a bitarray with true only at idx.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.calculate_WS!-Tuple{HierarchicalTemporalMemory.DistalSynapses,Any,Any,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.calculate_WS!","text":"calculate_WS!(pΠₛ,povp_Mₛ, α,B) finds the winning segments, growing new ones where necessary.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.countmap_empty-Tuple{Any}","page":"Reference","title":"HierarchicalTemporalMemory.countmap_empty","text":"countmap_empty(x) [Dict] Count the frequency of occurence for each element in x\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.decayS-Tuple{HierarchicalTemporalMemory.DistalSynapses,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.decayS","text":"decayS(s::DistalSynapses,pMₛ,α) are the dendritic segments that should decay according to LTD (long term depression). It's the segments that at the previous moment had enough potential synapses with active neurons to \"match\" the input (pMₛ), but didn't contribute to their neuron firing at this moment (they didn't activate strongly enough to depolarize it).\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.growseg!-Tuple{HierarchicalTemporalMemory.DistalSynapses,Array{Union{Nothing, Int64},1},Any}","page":"Reference","title":"HierarchicalTemporalMemory.growseg!","text":"growseg!(s::DistalSynapses,WS_col::Vector{Option{Int}},burstingcolidx) grows 1 new segment for each column with nothing as winning segment and replaces nothing with it. It resizes all the DistalSynapses matrices to append new dendritic segments. Foreach bursting minicolumn without a winning segment, the neuron to grow the segment is the one with the fewest segments.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.growsynapses!-Tuple{HierarchicalTemporalMemory.DistalSynapses,BitArray,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.growsynapses!","text":"growsynapses!(s::DistalSynapses, pWN::CellActivity,WS, povp_Mₛ) adds synapses between winning dendrites (WS) and a random sample of previously winning neurons (pWN).\n\nFor each dendrite, target neurons are selected among pWN with probability to pick each neuron determined by frac (mathitsynapseSampleSize - mathitprev_Overlap  length(mathitpWN)  (Bernoulli sampling)\n\nThis involves inserting new elements in random places of a SparseMatrixCSC and is the algorithm's performance bottleneck.\n\nTODO: try replacing CSC with a Dict implementation of sparse matrices.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.hcat!!-Tuple{SparseArrays.SparseMatrixCSC,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.hcat!!","text":"hcat!!(s::SparseMatrixCSC, I,V)\n\nWARNING: s is invalid after this operation!!! Its symbol should be reassigned to the output of hcat!!. Append new columns to s with 1 value V[c] at row I[c] each. Because SparseMatrixCSC is immutable, return a new, valid SparseMatrixCSC that points to the original's data structures. For large matrices, this is much faster than hcat.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.leastusedcell-Tuple{HierarchicalTemporalMemory.DistalSynapses,Any}","page":"Reference","title":"HierarchicalTemporalMemory.leastusedcell","text":"leastusedcell(s::DistalSynapses,col) finds the neuron of a minicolumn with the fewest dendrites.\n\nTODO: try Memoize.jl !\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.sp_activate-Tuple{SpatialPooler,Any}","page":"Reference","title":"HierarchicalTemporalMemory.sp_activate","text":"sp_activate(sp::SpatialPooler, z) calculates the SP's output activation for given input activation z.\n\nAlgorithm\n\nOverlap o(yᵢ)\nPropagate the input activations through the proximal synapses (matrix multiply)\nApply boosting factors b: control mechanism that spreads out the activation pattern across understimulated neurons (homeostatic excitability control)\nInhibition Z between yᵢ (local/global): competition where only the top-k yᵢ with the highest o(yᵢ) win; ensures sparsity The competition happens within an area around each neuron.\nk: number of winners depends on desired sparsity (s, see SPParams) and area size\nθ_inhibit: inhibition threshold per neighborhood = o(k-th y)\nZ(o(yᵢ)): convolution of o with θ_inhibit\nActivate winning yᵢ > activation threshold (θ_stimulus_activate)\n\ninfo: Info\nThe local sparsity s tends to the sparsity of the entire layer as it grows larger, but for small values or small inhibition radius it diverges, because of the limited & integral number of neurons winning in each neighborhood. This could be addressed by tie breaking, but it doesn't seem to have much practical importance.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.sparse_foreach-Tuple{Any,SparseArrays.SparseMatrixCSC,Any}","page":"Reference","title":"HierarchicalTemporalMemory.sparse_foreach","text":"sparse_foreach(f, s::SparseMatrixCSC,columnIdx) iterates over the columns of the sparse matrix s specified by columnIdx and applies f(columnElt,columnRowIdx) to each. Column elements are a @view into the nonzero elements at the given column of s.\n\nSee also: sparse_map\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.sparse_map-Tuple{Any,SparseArrays.SparseMatrixCSC,Any}","page":"Reference","title":"HierarchicalTemporalMemory.sparse_map","text":"sparse_map(f, s::SparseMatrixCSC,columnIdx) maps each column of the sparse matrix s specified by columnIdx to f(columnElt,columnRowIdx). Column elements are a @view into the nonzero elements at the given column of s.\n\nSee also: sparse_foreach\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.step_boost!-Tuple{SpatialPooler,Any}","page":"Reference","title":"HierarchicalTemporalMemory.step_boost!","text":"step_boost!(sp::SpatialPooler,a) evolves the boosting factors b (see sp_activate). They depend on:\n\nåₜ: moving average in time activation of each minicolumn\nåₙ: moving average in neighborhood activation of each minicolum\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.tm_activate-Tuple{TemporalMemory,Any}","page":"Reference","title":"HierarchicalTemporalMemory.tm_activate","text":"tm_activate(tm::TemporalMemory, c) calculates\n\nwhich minicolumns burst,\nwhich neurons in the layer are activated\nwhich become predictive\n\nfor minicolumn activation c (size Nc) given by the SpatialPooler. Uses also the previously predictive neurons Π (size Nₙ) from the TM's state.\n\nReturns\n\na: neuron activation (Nₙ)\nB: bursting minicolumns (Nc)\nWN: \"winning\" neurons (Nₙ)\n\nSee also: tm_predict, DistalSynapses\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.tm_predict-Tuple{TemporalMemory,Any}","page":"Reference","title":"HierarchicalTemporalMemory.tm_predict","text":"tm_predict(tm::TemporalMemory, α) calculates which neurons will be predictive at the next step given the currently active neurons α.\n\nReturns\n\nΠ: predictive neurons (Nₙ)\nΠₛ: predictive dendritic segments ('Nₛ') (caching)\nMₛ: matching dendritic segments (Nₛ) (learning)\novp_Mₛ: subthreshold-matching dendritic segments (Nₛ) (learning)\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.vcat!!-Tuple{SparseArrays.SparseMatrixCSC,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.vcat!!","text":"vcat!!(s::SparseMatrixCSC, J,V)\n\nWARNING: s is invalid after this operation!!! Its symbol should be reassigned to the output of vcat!!. Append new rows to s with 1 value V[r] at column J[r] each. Because SparseMatrixCSC is immutable, return a new, valid SparseMatrixCSC that points to the original's data structures. For large matrices, this is much faster than vcat.\n\n\n\n\n\n","category":"method"},{"location":"reference/#HierarchicalTemporalMemory.@percolumn-NTuple{4,Any}","page":"Reference","title":"HierarchicalTemporalMemory.@percolumn","text":"@percolumn(f,a,b,k)\n\nMacro to fold a large vector a per columns k and apply binary operator f elementwise\n\na: vector of size Nc*k, column-major (important because it will fold every k elements)\nb: vector of size Nc\n\n\n\n\n\n","category":"macro"},{"location":"reference/#HierarchicalTemporalMemory.@percolumn-Tuple{Any,Any,Any}","page":"Reference","title":"HierarchicalTemporalMemory.@percolumn","text":"@percolumn(reduce,a,k)\n\nMacro to reduce(a) per column k.\n\n\n\n\n\n","category":"macro"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"CurrentModule = HierarchicalTemporalMemory","category":"page"},{"location":"#Hierarchical-Temporal-Memory-1","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"","category":"section"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"Hierarchical Temporal Memory is an abstract algorithmic model of the human brain (specifically the neocortex). It's a tool for","category":"page"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"neuroscience: understanding the human brain\nmachine learning: predicting time series and detecting anomalies","category":"page"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"The main algorithms of this model, the Spatial Pooler and Temporal (Sequence) Memory, are described in:","category":"page"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"The HTM Spatial Pooler—A Neocortical Algorithm for Online Sparse Distributed Coding\nContinuous Online Sequence Learning with an Unsupervised Neural Network Model (section 3.3)","category":"page"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"The official implementation of this model in Python (NUPIC) serves as a reference point and as source of truth for many implementation details, but it doesn't take advantage of the data-driven design that the source material encourages and ends up quite verbose. This implementation uses Julia's expressivity to remain faithful to the papers' terminology, attempting to express the algorithms more simply and concisely, and thus instigate further research on them. Some essential features of this expressivity are broadcasting, duck-typed interfaces and unicode source code.","category":"page"},{"location":"#What-is-the-HTM?-1","page":"Hierarchical Temporal Memory","title":"What is the HTM?","text":"","category":"section"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"The Hierarchical Temporal Memory (HTM) is a biologically constrained theory, aiming primarily to model the function of the neocortex (a structure of the human brain), and as a secondary goal, machine learning applications. The algorithms expose many fundamental properties which are used to test this implementation.","category":"page"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"In contrast to many established neural network models, the HTM neuron performs coincidence detection across multiple input \"dendrites\". HTM is actually closer to a spiking neural network, with binary synapses and signals, that encodes information in population codes. Spatial pooling and the temporal memory are both unsupervised processes that adapt continuously; together, they learn to identify sequences in noisy input time series.","category":"page"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"HTM theory is not yet complete, lacking a definitive way to stabilize sequence representations and compose small models. Exactly for this reason, we believe that a concise and high level model can accelerate the research.","category":"page"},{"location":"#Roadmap-1","page":"Hierarchical Temporal Memory","title":"Roadmap","text":"","category":"section"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"Timeseries prediction tests, NAB results\nSensorimotor inference and temporal pooling ideas, influenced by forum discussions such as this. This goes in the direction of the biggest current problems:\ntemporal noise\nhierarchical model composition","category":"page"},{"location":"#Acknowledgements-1","page":"Hierarchical Temporal Memory","title":"Acknowledgements","text":"","category":"section"},{"location":"#","page":"Hierarchical Temporal Memory","title":"Hierarchical Temporal Memory","text":"Maria Litsa: logo and poster art","category":"page"},{"location":"poster/#JuliaCon-2020-poster-1","page":"JuliaCon 2020 poster","title":"JuliaCon 2020 poster","text":"","category":"section"},{"location":"poster/#","page":"JuliaCon 2020 poster","title":"JuliaCon 2020 poster","text":"This package is presented with a poster at JuliaCon 2020 (link to high-def pdf).","category":"page"},{"location":"poster/#","page":"JuliaCon 2020 poster","title":"JuliaCon 2020 poster","text":"<img src=\"https://raw.githubusercontent.com/Oblynx/HierarchicalTemporalMemory.jl/master/docs/src/assets/HTM.jl_%20Poster%20JuliaCon%202020.png\" alt=\"JuliaCon 2020 poster\"/>","category":"page"}]
}
