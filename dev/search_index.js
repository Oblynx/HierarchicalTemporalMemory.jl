var documenterSearchIndex = {"docs":
[{"location":"#","page":"Home","title":"Home","text":"CurrentModule = HierarchicalTemporalMemory","category":"page"},{"location":"#HierarchicalTemporalMemory-1","page":"Home","title":"HierarchicalTemporalMemory","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Modules = [HierarchicalTemporalMemory]","category":"page"},{"location":"#HierarchicalTemporalMemory.Hypercube","page":"Home","title":"HierarchicalTemporalMemory.Hypercube","text":"Hypercube iterates over a hypercube of radius Œ≥ around x·∂ú, bounded inside the space {1..sz}·¥∫.\n\nExamples\n\njulia> hc= Hypercube((3,3),1,(10,10));\n\njulia> foreach(println, hc)\n(2, 2)\n(3, 2)\n(4, 2)\n(2, 3)\n(3, 3)\n(4, 3)\n(2, 4)\n(3, 4)\n(4, 4)\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.Hypersphere","page":"Home","title":"HierarchicalTemporalMemory.Hypersphere","text":"Hypersphere is approximated with a Hypercube for simplicity\n\nSee also: Hypercube\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.SPParams","page":"Home","title":"HierarchicalTemporalMemory.SPParams","text":"SPParams{Nin,Nsp} holds the algorithm parameters for a spatial pooler with nomenclature similar to source\n\nThe dimension parameters are problem-specific and should be the first to be specified.\n\nThe tuning parameters have sensible defaults, which should be . All gated features are enabled by default.\n\nParameters\n\nDimensions\n\nsz·µ¢‚Çô, sz‚Çõ‚Çö: input/output dimensions\nŒ≥: receptive field radius (how large an input area an output minicolumn maps to)\n\nAlgorithm tuning\n\ns: average output sparsity\nprob_synapse: probability for each element of the sz·µ¢‚Çô √ó sz‚Çõ‚Çö space to be a synapse. Elements that roll below this value don't form a synapse and don't get a permanence value. If this is very low, the proximal synapses matrix can become sparse.\nŒ∏_permanence01: synapse permanence connection threshold\np‚Å∫_01,p‚Åª_01: synapse permanence adaptation rate (see ProximalSynapses)\nŒ∏_stimulus_activate: minicolumn absolute activation threshold\nTboost: boosting mechanism's moving average filter period\nŒ≤: boosting strength\n\nFeature gates\n\nenable_local_inhibit\nenable_learning\nenable_boosting\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.SpatialPooler","page":"Home","title":"HierarchicalTemporalMemory.SpatialPooler","text":"SpatialPooler{Nin,Nsp} is a learning algorithm that decorrelates the features of an input space, producing a Sparse Distributed Representation (SDR) of the input space. If defines the proximal connections of an HTM layer.\n\nExamples\n\nsp= SpatialPooler(SPParams(sz·µ¢‚Çô=(600,), sz‚Çõ‚Çö=(2048,)))\nz= Random.bitrand(600)  # input\nactivation= sp(z)\n# or, to adapt the SP to the input as well:\nactivation= step!(sp,z)\n\nProperties\n\nIt's called SpatialPooler because input patterns that share a large number of co-active neurons (i.e., that are spatially similar) are grouped together into a common output representation. It is designed to achieve a set of computational properties that support further downstream computations with SDRs, including:\n\npreserving topology of the input space by mapping similar inputs to similar outputs\ncontinuously adapting to changing statistics of the input stream\nforming fixed sparsity representations\nbeing robust to noise\nbeing fault tolerant\n\nSource\n\nAlgorithm overview\n\nMapping I/O spaces\n\nThe spatial pooler maps an input space x to an output space y of minicolumns through a matrix of proximal synapses. The input space can optionally have a topology, which the spatial pooler will preserve by mapping output minicolumn y·µ¢ to a subset of the input space, a Hypercube around center x·∂ú. julia x·∂ú(y·µ¢)= floor.(Int, (y·µ¢.-1) .* (sz·µ¢‚Çô./sz‚Çõ‚Çö)) .+1`\n\nOutput activation\n\nCalculate the overlap o(y·µ¢) by propagating the input activations through the proximal synapses and adjusting by boosting factors b (control mechanism that spreads out the activation pattern across understimulated neurons)\nInhibition Z between y·µ¢ (local/global): competition where only the top-K y·µ¢ with the highest o(y·µ¢) win; ensures sparsity\nActivate winning y·µ¢ > activation threshold (Œ∏_stimulus_activate)\n\nSee also: sp_activate\n\nState variables\n\nsynapses: includes the synapse permanence matrix D‚Çö\n√•‚Çú: [boosting] moving (in time) average (in time) activation of each minicolumn\n√•‚Çô: [boosting] moving (in time) average (in neighborhood) activation of each minicolumn\nœÜ: the adaptible radius of local inhibition\n\n\n\nSee also: ProximalSynapses, TemporalMemory\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.SpatialPooler-Tuple{Any}","page":"Home","title":"HierarchicalTemporalMemory.SpatialPooler","text":"Return the activation pattern of the SpatialPooler for the given input activation.\n\nz: CellActivity\n\nExample\n\nsp= SpatialPooler(SPParams(sz·µ¢‚Çô=(600,), sz‚Çõ‚Çö=(2048,)))\nz= Random.bitrand(600)  # input\nactivation= sp(z)\n# or, to adapt the SP to the input as well:\nactivation= step!(sp,z)\n\nFor details see: sp_activate\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.TMParams","page":"Home","title":"HierarchicalTemporalMemory.TMParams","text":"TMParams holds the algorithm parameters for a Temporal Memory with nomenclature similar to source\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.TemporalMemory","page":"Home","title":"HierarchicalTemporalMemory.TemporalMemory","text":"TemporalMemory learns to predict sequences of input Sparse Distributed Representations (SDRs), usually generated by a SpatialPooler. It learns to represent each input symbol in the temporal context of the symbols that come before it in the sequence, using the individual neurons of each minicolumn and their distal synapses.\n\nWhen considering a neuron layer with proximal and distal synapses, the spatial pooler is a way to activate and learn the proximal synapses, while the temporal memory is a way to activate and learn the distal synapses.\n\nTM activation\n\nAn SDR input activates some minicolumns of the neuron layer.\n\n\n\nSee also: TMState\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.predict!-Tuple{SDRClassifier,Any,Int64}","page":"Home","title":"HierarchicalTemporalMemory.predict!","text":"predict!(classifier::SDRClassifier, Œ†,target::Int; enable_learning=true)=\n\nPredict probability that the output represents each bucket of an arithmetic encoder. 0<=predict!<=1\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.reverse_simpleArithmetic-Tuple{Array{T,1} where T,Any,Any}","page":"Home","title":"HierarchicalTemporalMemory.reverse_simpleArithmetic","text":"reverse_simpleArithmetic(bucketProbDist, algorithm,params)\n\nReverses the simple arithmetic encoder, given the same parameters. Inputs a probability distribution across all buckets and collapses it to a single arithmetic value, representing the most likely estimation in the timeseries' domain (like a defuzzifier)\n\nArguments\n\nbucketProbDist [nbucket]: discrete probability of each bucket [0-1]\nalgorithm: {'mean','mode','highmean'} 'highmean' is the mean of the highest-estimated values\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.step!-Tuple{HierarchicalTemporalMemory.ProximalSynapses,Any,Any,Any}","page":"Home","title":"HierarchicalTemporalMemory.step!","text":"step!(s::ProximalSynapses, z,a, params) adapts the proximal synapses' permanences with a hebbian learning rule on input z and activation a. The adaptation has a causal and an anticausal component:\n\nIf the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened\nIf the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened\n\nSee alse: ProximalSynapses\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.step!-Tuple{SpatialPooler,Any}","page":"Home","title":"HierarchicalTemporalMemory.step!","text":"step!(sp::SpatialPooler, z::CellActivity) evolves the Spatial Pooler to the next timestep by evolving each of its constituents (synapses, boosting, inhibition radius) and returns the output activation.\n\nSee also: sp_activate\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.ùïäùï¢range","page":"Home","title":"HierarchicalTemporalMemory.ùïäùï¢range","text":"ùïäùï¢range is the domain of connection permanence quantization\n\n\n\n\n\n","category":"constant"},{"location":"#HierarchicalTemporalMemory.CellActivity","page":"Home","title":"HierarchicalTemporalMemory.CellActivity","text":"Type of neuron layer activations\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.InhibitionRadius","page":"Home","title":"HierarchicalTemporalMemory.InhibitionRadius","text":"The inhibition radius of a Spatial Pooler's columns is a dynamical system that evolves under the influence of other elements of the Spatial Pooler. It provides an init (constructor) and a step! function.\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.LengthfulIter","page":"Home","title":"HierarchicalTemporalMemory.LengthfulIter","text":"Iterator transformations, like filters, don't keep the length info. LengthfulIter is a wrapper that allows length info known programmatically to be used.\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.ProximalSynapses","page":"Home","title":"HierarchicalTemporalMemory.ProximalSynapses","text":"ProximalSynapses{SynapseT<:AnySynapses,ConnectedT<:AnyConnection} are the feedforward connections of 2 neuron layers.\n\nUsed in the context of the SpatialPooler.\n\nDescription\n\nThe neurons of both layers are expected to form minicolumns which share the same feedforward connections. The synapses are binary: they don't have a scalar weight, but either conduct (1) or not (0). Instead, they have a permanence value D‚Çö ‚àà (0,1] and a connection threshold Œ∏.\n\nInitialization\n\nLet presynaptic (input) neuron x·µ¢ and postsynaptic (output) neuron y·µ¢, and a topological I/O mapping x·µ¢(y·µ¢) := Hypercube(y·µ¢). ‚àÄ\n\nSynapse adaptation\n\nThey adapt with a hebbian learning rule. The adaptation has a causal and an anticausal component:\n\nIf the postsynaptic neuron fires and the presynaptic fired too, the synapse is strengthened\nIf the postsynaptic neuron fires, but the presynaptic didn't, the synapse is weakened\n\nThe synaptic permanences are clipped at the boundaries of 0 and 1.\n\nA simple implementation of the learning rule would look like this, where z: input, a: output\n\nlearn!(D‚Çö,z,a)= begin\n  D‚Çö[z,a]  .= (D‚Çö[z,a].>0) .* (D‚Çö[z,a]   .‚äï p‚Å∫)\n  D‚Çö[.!z,a].= (D‚Çö[z,a].>0) .* (D‚Çö[.!z,a] .‚äñ p‚Åª)\nend\n\nType parameters\n\nThey allow a dense or sparse matrix representation of the synapses\n\nSynapseT: DenseSynapses or SparseSynapses\nConnectedT: DenseConnection or SparseConnection\n\nSee also: DistalSynapses, SpatialPooler, TemporalMemory\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.Sequence","page":"Home","title":"HierarchicalTemporalMemory.Sequence","text":"Sequence(;Œ¥,init) is an easy way to define sequences with transition function Œ¥ and starting condition init as an iterator.\n\nExamples\n\nFibonacci sequence:\n\nfibonacci_Œ¥(a,b)= b, a+b;\nfibonacci_init= 0,1;\nLazy.@as _1 HTM.Sequence(fibonacci_Œ¥, fibonacci_init) IterTools.take(_1, 13) foreach(println, _1)\n\n# output\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\n55\n89\n144\n\nFactorial sequence:\n\nfactorial_Œ¥(a,b)= a*b, b+1;\nfactorial_init= 1,1;\nLazy.@as _1 HTM.Sequence(factorial_Œ¥, factorial_init) IterTools.take(_1, 13) foreach(println, _1)\n\n# output\n1\n1\n2\n6\n24\n120\n720\n5040\n40320\n362880\n3628800\n39916800\n479001600\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.Truesof","page":"Home","title":"HierarchicalTemporalMemory.Truesof","text":"Iterate over the trues of a BitArray\n\nExamples\n\njulia> b= Random.bitrand(5)\n5-element BitArray{1}:\n 1\n 0\n 0\n 1\n 1\n\njulia> foreach(i-> print(string(i)*\" \"), HTM.Truesof(b))\n1 4 5\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.ùïäùï¢","page":"Home","title":"HierarchicalTemporalMemory.ùïäùï¢","text":"ùïäùï¢ is the type of connection permanences, defining their quantization domain\n\n\n\n\n\n","category":"type"},{"location":"#HierarchicalTemporalMemory.bitarray-Tuple{Any,Any}","page":"Home","title":"HierarchicalTemporalMemory.bitarray","text":"bitarray(dims, idx)\n\nCreate a bitarray with true only at idx.\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.hcat!!-Tuple{SparseArrays.SparseMatrixCSC,Any,Any}","page":"Home","title":"HierarchicalTemporalMemory.hcat!!","text":"hcat!!(s::SparseMatrixCSC, I,V)\n\nWARNING: s is invalid after this operation!!! Its symbol should be reassigned to the output of hcat!!. Append new columns to s with 1 value V[c] at row I[c] each. Because SparseMatrixCSC is immutable, return a new, valid SparseMatrixCSC that points to the original's data structures. For large matrices, this is much faster than hcat.\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.sp_activate-Union{Tuple{Nsp}, Tuple{Nin}, Tuple{SpatialPooler{Nin,Nsp},Any}} where Nsp where Nin","page":"Home","title":"HierarchicalTemporalMemory.sp_activate","text":"sp_activate(sp::SpatialPooler{Nin,Nsp}, z) calculates the SP's output activation for given input activation z.\n\nAlgorithm\n\nOverlap o(y·µ¢)\nPropagate the input activations through the proximal synapses (matrix multiply)\nApply boosting factors b: control mechanism that spreads out the activation pattern across understimulated neurons (homeostatic excitability control)\nInhibition Z between y·µ¢ (local/global): competition where only the top-k y·µ¢ with the highest o(y·µ¢) win; ensures sparsity The competition happens within an area around each neuron.\nk: number of winners depends on desired sparsity (s, see SPParams) and area size\nŒ∏_inhibit: inhibition threshold per neighborhood = o(k-th y)\nZ(o(y·µ¢)): convolution of o with Œ∏_inhibit\nActivate winning y·µ¢ > activation threshold (Œ∏_stimulus_activate)\n\ninfo: Info\nThe local sparsity s tends to the sparsity of the entire layer as it grows larger, but for small values or small inhibition radius it diverges, because of the limited & integral number of neurons winning in each neighborhood. This could be addressed by tie breaking, but it doesn't seem to have much practical importance.\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.sparse_foreach-Tuple{Any,SparseArrays.SparseMatrixCSC,Any}","page":"Home","title":"HierarchicalTemporalMemory.sparse_foreach","text":"sparse_foreach(f, s::SparseMatrixCSC,columnIdx)\n\nIterate SparseMatrix s and apply f columnwise (f(s,nzrange,rowvals))\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.step_boost!-Tuple{SpatialPooler,Any}","page":"Home","title":"HierarchicalTemporalMemory.step_boost!","text":"step_boost!(sp::SpatialPooler,a) evolves the boosting factors b (see sp_activate). They depend on:\n\n√•‚Çú: moving average in time activation of each minicolumn\n√•‚Çô: moving average in neighborhood activation of each minicolum\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.vcat!!-Tuple{SparseArrays.SparseMatrixCSC,Any,Any}","page":"Home","title":"HierarchicalTemporalMemory.vcat!!","text":"vcat!!(s::SparseMatrixCSC, J,V)\n\nWARNING: s is invalid after this operation!!! Its symbol should be reassigned to the output of vcat!!. Append new rows to s with 1 value V[r] at column J[r] each. Because SparseMatrixCSC is immutable, return a new, valid SparseMatrixCSC that points to the original's data structures. For large matrices, this is much faster than vcat.\n\n\n\n\n\n","category":"method"},{"location":"#HierarchicalTemporalMemory.@percolumn-NTuple{4,Any}","page":"Home","title":"HierarchicalTemporalMemory.@percolumn","text":"@percolumn(f,a,b,k,Ncol)\n\nMacro to apply f elementwise and concatenate the results.\n\na: vector of size [Ncol*k], column-major\nb: vectors of size Ncol\n\n\n\n\n\n","category":"macro"},{"location":"#HierarchicalTemporalMemory.@percolumn-Tuple{Any,Any,Any}","page":"Home","title":"HierarchicalTemporalMemory.@percolumn","text":"@percolumn(reduce,a,k,Ncol)\n\nMacro to reduce a per column.\n\n\n\n\n\n","category":"macro"}]
}
